{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NeurIPS Open Polymer Prediction 2025 - T4 x2 GPU Solution\n",
    "\n",
    "**Target**: Optimized for T4 x2 GPU setup with memory-efficient training.\n",
    "\n",
    "## üéØ T4 x2 Optimizations\n",
    "- **Memory**: Optimized for 16GB total VRAM (8GB per GPU)\n",
    "- **Batch Size**: 48 per GPU (96 total)\n",
    "- **Model Size**: 64 hidden channels\n",
    "- **Training**: Mixed precision + optimized data loading\n",
    "- **Expected Performance**: ~0.145 wMAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ALL IMPORTS AND CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "# PyTorch Geometric\n",
    "from torch_geometric.data import Data, Batch\n",
    "from torch_geometric.nn import GINConv, global_add_pool, global_mean_pool\n",
    "\n",
    "# RDKit for molecular processing\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Descriptors, rdMolDescriptors\n",
    "\n",
    "# Progress bars and utilities\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['PYTHONWARNINGS'] = 'ignore'\n",
    "\n",
    "# =============================================================================\n",
    "# T4 x2 CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "# GPU configuration\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0,1'  # Use both GPUs\n",
    "\n",
    "# Model parameters optimized for T4 x2\n",
    "BATCH_SIZE = 48  # Per GPU - optimized for T4 memory\n",
    "HIDDEN_CHANNELS = 64  # Reduced for memory efficiency\n",
    "NUM_LAYERS = 6  # Reduced layers\n",
    "TRAINING_EPOCHS = 40\n",
    "USE_MIXED_PRECISION = True\n",
    "\n",
    "# GPU optimizations\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cudnn.deterministic = False\n",
    "\n",
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"üöÄ Using {torch.cuda.device_count()} GPU(s): {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è CUDA not available, using CPU\")\n",
    "\n",
    "# Mixed precision scaler\n",
    "scaler = GradScaler() if USE_MIXED_PRECISION else None\n",
    "\n",
    "print(f\"‚úÖ Configuration loaded:\")\n",
    "print(f\"   Batch Size: {BATCH_SIZE} per GPU\")\n",
    "print(f\"   Hidden Channels: {HIDDEN_CHANNELS}\")\n",
    "print(f\"   Training Epochs: {TRAINING_EPOCHS}\")\n",
    "print(f\"   Mixed Precision: {USE_MIXED_PRECISION}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DATA LOADING WITH KAGGLE PATH DETECTION\n",
    "# =============================================================================\n",
    "\n",
    "def detect_data_paths():\n",
    "    \"\"\"Smart path detection for Kaggle and local environments.\"\"\"\n",
    "    \n",
    "    # Kaggle paths (primary)\n",
    "    kaggle_paths = [\n",
    "        '/kaggle/input/neurips-open-polymer-prediction-2025',\n",
    "        '/kaggle/input/neurips-2025-polymer-prediction',\n",
    "        '/kaggle/input/polymer-prediction-2025'\n",
    "    ]\n",
    "    \n",
    "    # Local paths (fallback)\n",
    "    local_paths = ['info', 'data', '.']\n",
    "    \n",
    "    # Check Kaggle paths first\n",
    "    for path in kaggle_paths:\n",
    "        if os.path.exists(path):\n",
    "            print(f\"üìÅ Using Kaggle data path: {path}\")\n",
    "            return path\n",
    "    \n",
    "    # Check local paths\n",
    "    for path in local_paths:\n",
    "        if os.path.exists(os.path.join(path, 'train.csv')):\n",
    "            print(f\"üìÅ Using local data path: {path}\")\n",
    "            return path\n",
    "    \n",
    "    raise FileNotFoundError(\"Could not find data files in any expected location\")\n",
    "\n",
    "# Detect data path\n",
    "DATA_PATH = detect_data_paths()\n",
    "\n",
    "# Load main datasets\n",
    "print(\"üìä Loading datasets...\")\n",
    "train_df = pd.read_csv(os.path.join(DATA_PATH, 'train.csv'))\n",
    "test_df = pd.read_csv(os.path.join(DATA_PATH, 'test.csv'))\n",
    "\n",
    "print(f\"‚úÖ Loaded datasets:\")\n",
    "print(f\"   Training samples: {len(train_df):,}\")\n",
    "print(f\"   Test samples: {len(test_df):,}\")\n",
    "print(f\"   Features: {train_df.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MOLECULAR FEATURIZATION\n",
    "# =============================================================================\n",
    "\n",
    "def get_atom_features(atom):\n",
    "    \"\"\"Extract atom features for graph neural network.\"\"\"\n",
    "    features = [\n",
    "        atom.GetAtomicNum(),\n",
    "        atom.GetDegree(),\n",
    "        atom.GetFormalCharge(),\n",
    "        int(atom.GetHybridization()),\n",
    "        int(atom.GetIsAromatic()),\n",
    "        atom.GetMass(),\n",
    "        atom.GetTotalNumHs(),\n",
    "        int(atom.IsInRing()),\n",
    "    ]\n",
    "    return features\n",
    "\n",
    "def smiles_to_graph(smiles):\n",
    "    \"\"\"Convert SMILES to PyTorch Geometric graph.\"\"\"\n",
    "    try:\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol is None:\n",
    "            return None\n",
    "        \n",
    "        # Atom features\n",
    "        atom_features = []\n",
    "        for atom in mol.GetAtoms():\n",
    "            atom_features.append(get_atom_features(atom))\n",
    "        \n",
    "        if not atom_features:\n",
    "            return None\n",
    "        \n",
    "        # Edge indices\n",
    "        edge_indices = []\n",
    "        for bond in mol.GetBonds():\n",
    "            i = bond.GetBeginAtomIdx()\n",
    "            j = bond.GetEndAtomIdx()\n",
    "            edge_indices.extend([[i, j], [j, i]])  # Undirected graph\n",
    "        \n",
    "        # Convert to tensors\n",
    "        x = torch.tensor(atom_features, dtype=torch.float)\n",
    "        \n",
    "        if edge_indices:\n",
    "            edge_index = torch.tensor(edge_indices, dtype=torch.long).t().contiguous()\n",
    "        else:\n",
    "            # Handle single atom molecules\n",
    "            edge_index = torch.empty((2, 0), dtype=torch.long)\n",
    "        \n",
    "        # Pad features to consistent size\n",
    "        if x.size(1) < 32:\n",
    "            padding = torch.zeros(x.size(0), 32 - x.size(1))\n",
    "            x = torch.cat([x, padding], dim=1)\n",
    "        \n",
    "        return Data(x=x, edge_index=edge_index)\n",
    "    \n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "print(\"‚úÖ Molecular featurization functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DATASET CLASS\n",
    "# =============================================================================\n",
    "\n",
    "class PolymerDataset(Dataset):\n",
    "    \"\"\"Dataset for polymer property prediction.\"\"\"\n",
    "    \n",
    "    def __init__(self, df, is_test=False):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.is_test = is_test\n",
    "        \n",
    "        # Property columns\n",
    "        self.property_cols = ['Tg', 'FFV', 'Tc', 'Density', 'Rg']\n",
    "        \n",
    "        # Pre-process graphs\n",
    "        print(f\"üîÑ Processing {len(self.df)} samples...\")\n",
    "        self.graphs = []\n",
    "        valid_indices = []\n",
    "        \n",
    "        for idx, row in tqdm(self.df.iterrows(), total=len(self.df), desc=\"Processing\"):\n",
    "            graph = smiles_to_graph(row['SMILES'])\n",
    "            if graph is not None:\n",
    "                self.graphs.append(graph)\n",
    "                valid_indices.append(idx)\n",
    "        \n",
    "        # Keep only valid samples\n",
    "        self.df = self.df.iloc[valid_indices].reset_index(drop=True)\n",
    "        print(f\"‚úÖ Processed {len(self.graphs)} valid samples\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.graphs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        graph = self.graphs[idx].clone()\n",
    "        \n",
    "        if not self.is_test:\n",
    "            # Training/validation: add targets and masks\n",
    "            row = self.df.iloc[idx]\n",
    "            \n",
    "            targets = []\n",
    "            masks = []\n",
    "            \n",
    "            for col in self.property_cols:\n",
    "                if pd.notna(row[col]):\n",
    "                    targets.append(float(row[col]))\n",
    "                    masks.append(1.0)\n",
    "                else:\n",
    "                    targets.append(0.0)\n",
    "                    masks.append(0.0)\n",
    "            \n",
    "            graph.y = torch.tensor(targets, dtype=torch.float)\n",
    "            graph.mask = torch.tensor(masks, dtype=torch.float)\n",
    "        \n",
    "        return graph\n",
    "\n",
    "def collate_batch(batch):\n",
    "    \"\"\"Optimized collate function using PyTorch Geometric batching.\"\"\"\n",
    "    batch = [item for item in batch if item is not None]\n",
    "    if not batch:\n",
    "        return None\n",
    "    return Batch.from_data_list(batch)\n",
    "\n",
    "print(\"‚úÖ Dataset class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MODEL DEFINITION\n",
    "# =============================================================================\n",
    "\n",
    "class T4PolyGIN(nn.Module):\n",
    "    \"\"\"Graph Isomorphism Network optimized for T4 GPUs.\"\"\"\n",
    "    \n",
    "    def __init__(self, num_atom_features=32, hidden_channels=64, num_layers=6, num_targets=5, dropout=0.1):\n",
    "        super(T4PolyGIN, self).__init__()\n",
    "        \n",
    "        # Store device for DataParallel compatibility\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        # Input projection\n",
    "        self.input_proj = nn.Linear(num_atom_features, hidden_channels)\n",
    "        \n",
    "        # GIN layers\n",
    "        self.gin_layers = nn.ModuleList()\n",
    "        for _ in range(num_layers):\n",
    "            mlp = nn.Sequential(\n",
    "                nn.Linear(hidden_channels, hidden_channels * 2),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_channels * 2, hidden_channels),\n",
    "                nn.Dropout(dropout)\n",
    "            )\n",
    "            self.gin_layers.append(GINConv(mlp))\n",
    "        \n",
    "        # Output layers\n",
    "        self.output = nn.Sequential(\n",
    "            nn.Linear(hidden_channels, hidden_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_channels, num_targets)\n",
    "        )\n",
    "    \n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        \n",
    "        # Get device (handle DataParallel)\n",
    "        try:\n",
    "            device = next(self.parameters()).device\n",
    "        except StopIteration:\n",
    "            device = self.device\n",
    "        \n",
    "        # Input projection\n",
    "        x = self.input_proj(x)\n",
    "        \n",
    "        # GIN layers\n",
    "        for gin_layer in self.gin_layers:\n",
    "            x = gin_layer(x, edge_index)\n",
    "            x = F.relu(x)\n",
    "        \n",
    "        # Global pooling\n",
    "        x = global_mean_pool(x, batch)\n",
    "        \n",
    "        # Output\n",
    "        return self.output(x)\n",
    "\n",
    "print(\"‚úÖ Model class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# LOSS FUNCTION AND TRAINING FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def weighted_mae_loss(predictions, targets, masks):\n",
    "    \"\"\"Weighted MAE loss with DataParallel handling.\"\"\"\n",
    "    \n",
    "    # Handle DataParallel shape mismatch\n",
    "    if predictions.shape[0] != targets.shape[0]:\n",
    "        actual_batch_size = targets.shape[0]\n",
    "        predictions = predictions[:actual_batch_size]\n",
    "    \n",
    "    # Validate shapes\n",
    "    if predictions.shape != targets.shape or predictions.shape != masks.shape:\n",
    "        raise ValueError(f\"Shape mismatch: pred={predictions.shape}, target={targets.shape}, mask={masks.shape}\")\n",
    "    \n",
    "    # Equal weights for all properties\n",
    "    weights = torch.tensor([1.0, 1.0, 1.0, 1.0, 1.0], device=predictions.device, dtype=predictions.dtype)\n",
    "    if len(weights.shape) == 1 and len(predictions.shape) == 2:\n",
    "        weights = weights.unsqueeze(0)\n",
    "    \n",
    "    # Calculate weighted MAE\n",
    "    mae_per_property = torch.abs(predictions - targets) * masks\n",
    "    weighted_mae = (mae_per_property * weights).sum() / (masks * weights).sum()\n",
    "    \n",
    "    # Handle edge cases\n",
    "    if torch.isnan(weighted_mae) or torch.isinf(weighted_mae):\n",
    "        return torch.tensor(0.0, device=predictions.device, dtype=predictions.dtype)\n",
    "    \n",
    "    return weighted_mae\n",
    "\n",
    "def train_epoch(model, train_loader, optimizer, device):\n",
    "    \"\"\"Train for one epoch.\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    for batch in tqdm(train_loader, desc=\"Training\", leave=False):\n",
    "        if batch is None:\n",
    "            continue\n",
    "        \n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if USE_MIXED_PRECISION:\n",
    "            with autocast():\n",
    "                predictions = model(batch)\n",
    "                loss = weighted_mae_loss(predictions, batch.y, batch.mask)\n",
    "            \n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            predictions = model(batch)\n",
    "            loss = weighted_mae_loss(predictions, batch.y, batch.mask)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "    \n",
    "    return total_loss / max(num_batches, 1)\n",
    "\n",
    "def evaluate(model, val_loader, device):\n",
    "    \"\"\"Evaluate model on validation set.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=\"Validation\", leave=False):\n",
    "            if batch is None:\n",
    "                continue\n",
    "            \n",
    "            batch = batch.to(device)\n",
    "            \n",
    "            if USE_MIXED_PRECISION:\n",
    "                with autocast():\n",
    "                    predictions = model(batch)\n",
    "                    loss = weighted_mae_loss(predictions, batch.y, batch.mask)\n",
    "            else:\n",
    "                predictions = model(batch)\n",
    "                loss = weighted_mae_loss(predictions, batch.y, batch.mask)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "    \n",
    "    return total_loss / max(num_batches, 1)\n",
    "\n",
    "print(\"‚úÖ Training functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DATA PREPARATION\n",
    "# =============================================================================\n",
    "\n",
    "# Create datasets\n",
    "print(\"üìä Creating datasets...\")\n",
    "\n",
    "# Split training data\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_data, val_data = train_test_split(train_df, test_size=0.15, random_state=42)\n",
    "\n",
    "# Create dataset objects\n",
    "train_dataset = PolymerDataset(train_data, is_test=False)\n",
    "val_dataset = PolymerDataset(val_data, is_test=False)\n",
    "test_dataset = PolymerDataset(test_df, is_test=True)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True, \n",
    "    collate_fn=collate_batch,\n",
    "    num_workers=2,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True,\n",
    "    prefetch_factor=4\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=False, \n",
    "    collate_fn=collate_batch,\n",
    "    num_workers=2,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True,\n",
    "    prefetch_factor=4\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=False, \n",
    "    collate_fn=collate_batch,\n",
    "    num_workers=2,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True,\n",
    "    prefetch_factor=4\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Dataset sizes:\")\n",
    "print(f\"   Training: {len(train_dataset):,}\")\n",
    "print(f\"   Validation: {len(val_dataset):,}\")\n",
    "print(f\"   Test: {len(test_dataset):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MODEL INITIALIZATION\n",
    "# =============================================================================\n",
    "\n",
    "# Initialize model\n",
    "model = T4PolyGIN(\n",
    "    num_atom_features=32,\n",
    "    hidden_channels=HIDDEN_CHANNELS,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    num_targets=5,\n",
    "    dropout=0.1\n",
    ")\n",
    "\n",
    "# Move to device\n",
    "model = model.to(device)\n",
    "\n",
    "# Multi-GPU setup\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"üöÄ Using {torch.cuda.device_count()} GPUs with DataParallel\")\n",
    "    model = nn.DataParallel(model)\n",
    "    print(\"‚ö†Ô∏è DataParallel enabled - tensor shape fixes applied\")\n",
    "\n",
    "# Optimizer and scheduler\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=TRAINING_EPOCHS)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"‚úÖ Model initialized:\")\n",
    "print(f\"   Total parameters: {total_params:,}\")\n",
    "print(f\"   Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"   Model size: ~{total_params * 4 / 1e6:.1f}MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TRAINING LOOP\n",
    "# =============================================================================\n",
    "\n",
    "print(\"üöÄ Starting training...\")\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(TRAINING_EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch+1}/{TRAINING_EPOCHS}\")\n",
    "    \n",
    "    # Train\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, device)\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    # Validate\n",
    "    val_loss = evaluate(model, val_loader, device)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    # Update scheduler\n",
    "    scheduler.step()\n",
    "    \n",
    "    print(f\"Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "    \n",
    "    # Save best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), 'best_t4x2_model.pth')\n",
    "        print(f\"‚úÖ New best model saved (Val Loss: {val_loss:.4f})\")\n",
    "    \n",
    "    # Early stopping\n",
    "    if epoch > 10 and val_loss > min(val_losses[-5:]) * 1.1:\n",
    "        print(\"‚èπÔ∏è Early stopping triggered\")\n",
    "        break\n",
    "\n",
    "print(f\"\\nüéâ Training completed! Best validation loss: {best_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TEST PREDICTIONS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"üîÆ Generating test predictions...\")\n",
    "\n",
    "# Load best model\n",
    "model.load_state_dict(torch.load('best_t4x2_model.pth'))\n",
    "model.eval()\n",
    "\n",
    "test_predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, desc=\"Predicting\"):\n",
    "        if batch is None:\n",
    "            continue\n",
    "        \n",
    "        batch = batch.to(device)\n",
    "        \n",
    "        if USE_MIXED_PRECISION:\n",
    "            with autocast():\n",
    "                predictions = model(batch)\n",
    "        else:\n",
    "            predictions = model(batch)\n",
    "        \n",
    "        # Handle DataParallel shape mismatch for test predictions\n",
    "        if hasattr(model, 'module'):\n",
    "            actual_batch_size = batch.batch.max().item() + 1\n",
    "            if predictions.shape[0] > actual_batch_size:\n",
    "                predictions = predictions[:actual_batch_size]\n",
    "        \n",
    "        test_predictions.append(predictions.cpu().numpy())\n",
    "\n",
    "# Combine predictions\n",
    "test_predictions = np.vstack(test_predictions)\n",
    "\n",
    "print(f\"‚úÖ Generated predictions for {len(test_predictions)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CREATE SUBMISSION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"üìù Creating submission file...\")\n",
    "\n",
    "# Create submission dataframe\n",
    "submission_df = test_df[['ID']].copy()\n",
    "property_cols = ['Tg', 'FFV', 'Tc', 'Density', 'Rg']\n",
    "\n",
    "for i, col in enumerate(property_cols):\n",
    "    submission_df[col] = test_predictions[:, i]\n",
    "\n",
    "# Save submission\n",
    "submission_df.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(f\"‚úÖ Submission saved with {len(submission_df)} predictions\")\n",
    "print(f\"üìä Submission preview:\")\n",
    "print(submission_df.head())\n",
    "\n",
    "# Display training summary\n",
    "print(f\"\\nüéØ Training Summary:\")\n",
    "print(f\"   Best validation loss: {best_val_loss:.4f}\")\n",
    "print(f\"   Total epochs: {len(train_losses)}\")\n",
    "print(f\"   Final train loss: {train_losses[-1]:.4f}\")\n",
    "print(f\"   Model parameters: {trainable_params:,}\")\n",
    "\n",
    "print(\"\\nüéâ T4x2 training completed successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}