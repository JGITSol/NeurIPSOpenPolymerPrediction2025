# NeurIPS Open Polymer Prediction 2025 - GPU Enhanced Solution
# Competition-Ready Single File Implementation

# =============================================================================
# CONFIGURATION & SETUP
# =============================================================================

# Configuration
AUTO_MODE = True  # Set to False for manual step-by-step execution
DEBUG_MODE = True  # Enable detailed logging
USE_GPU = True    # Set to False to force CPU usage

# Competition parameters
PRETRAINING_EPOCHS = 10
TRAINING_EPOCHS = 50
BATCH_SIZE = 48  # Optimized for 6GB VRAM
HIDDEN_CHANNELS = 96
NUM_LAYERS = 8

print("ðŸš€ NeurIPS Open Polymer Prediction 2025 - GPU Enhanced Solution")
print(f"Mode: {'AUTO' if AUTO_MODE else 'MANUAL'} | Debug: {DEBUG_MODE} | GPU: {USE_GPU}")
print("=" * 80)

# =============================================================================
# DEPENDENCY INSTALLATION & IMPORTS
# =============================================================================

import subprocess
import sys
import os
import warnings
warnings.filterwarnings('ignore')

def install_package(package, check_import=None):
    """Install package if not already installed."""
    try:
        if check_import:
            __import__(check_import)
        else:
            __import__(package)
        if DEBUG_MODE:
            print(f"âœ… {package} already installed")
        return True
    except ImportError:
        print(f"ðŸ“¦ Installing {package}...")
        try:
            subprocess.check_call([sys.executable, "-m", "pip", "install", package])
            print(f"âœ… {package} installed successfully")
            return True
        except subprocess.CalledProcessError as e:
            print(f"âŒ Failed to install {package}: {e}")
            return False

# Install core dependencies
packages = [
    ("torch", "torch"),
    ("torch-geometric", "torch_geometric"), 
    ("rdkit-pypi", "rdkit"),
    ("pandas", "pandas"),
    ("numpy", "numpy"),
    ("scikit-learn", "sklearn"),
    ("lightgbm", "lightgbm"),
    ("tqdm", "tqdm"),
    ("matplotlib", "matplotlib"),
    ("seaborn", "seaborn")
]

print("ðŸ“¦ Checking and installing dependencies...")
for package, import_name in packages:
    install_package(package, import_name)

# Import all required libraries
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from torch_geometric.data import Data, Batch
from torch_geometric.nn import GINConv, global_add_pool, global_mean_pool
from torch_geometric.transforms import Compose, AddSelfLoops
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, KFold
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_absolute_error
import lightgbm as lgb
from rdkit import Chem
from rdkit.Chem import Descriptors, Crippen, Lipinski
import random
from tqdm import tqdm
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import json

# Set random seeds for reproducibility
def set_seeds(seed=42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)

set_seeds(42)

# Check GPU availability
device = torch.device('cuda' if torch.cuda.is_available() and USE_GPU else 'cpu')
print(f"ðŸ”§ Device: {device}")
if torch.cuda.is_available() and USE_GPU:
    print(f"ðŸŽ® GPU: {torch.cuda.get_device_name(0)}")
    print(f"ðŸ’¾ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")

print("âœ… Environment setup complete!")
print("=" * 80)# 
=============================================================================
# DATA LOADING & PREPROCESSING
# =============================================================================

print("ðŸ“Š Loading competition data...")

# Load data
try:
    train_df = pd.read_csv('info/train.csv')
    test_df = pd.read_csv('info/test.csv')
    print(f"âœ… Training data: {len(train_df)} samples")
    print(f"âœ… Test data: {len(test_df)} samples")
except FileNotFoundError as e:
    print(f"âŒ Data files not found: {e}")
    print("Please ensure train.csv and test.csv are in the 'info/' directory")
    sys.exit(1)

# Target columns
target_columns = ['Tg', 'FFV', 'Tc', 'Density', 'Rg']

# Check missing values
print("\nMissing values per target:")
for col in target_columns:
    missing = train_df[col].isna().sum()
    total = len(train_df)
    print(f"  {col}: {missing}/{total} ({missing/total*100:.1f}%)")

# =============================================================================
# ENHANCED MOLECULAR FEATURIZATION
# =============================================================================

def get_enhanced_atom_features(atom):
    """Get enhanced atom features (177 dimensions)."""
    features = []
    
    # Basic atom properties
    features.append(atom.GetAtomicNum())
    features.append(atom.GetDegree())
    features.append(atom.GetFormalCharge())
    features.append(atom.GetHybridization().real)
    features.append(atom.GetImplicitValence())
    features.append(atom.GetIsAromatic())
    features.append(atom.GetNoImplicit())
    features.append(atom.GetNumExplicitHs())
    features.append(atom.GetNumImplicitHs())
    features.append(atom.GetNumRadicalElectrons())
    features.append(atom.GetTotalDegree())
    features.append(atom.GetTotalNumHs())
    features.append(atom.GetTotalValence())
    features.append(atom.IsInRing())
    features.append(atom.IsInRingSize(3))
    features.append(atom.IsInRingSize(4))
    features.append(atom.IsInRingSize(5))
    features.append(atom.IsInRingSize(6))
    features.append(atom.IsInRingSize(7))
    features.append(atom.IsInRingSize(8))
    
    # One-hot encoding for common atoms
    atom_types = ['C', 'N', 'O', 'S', 'F', 'Si', 'P', 'Cl', 'Br', 'Mg', 'Na', 'Ca', 'Fe', 'As', 'Al', 'I', 'B', 'V', 'K', 'Tl', 'Yb', 'Sb', 'Sn', 'Ag', 'Pd', 'Co', 'Se', 'Ti', 'Zn', 'H', 'Li', 'Ge', 'Cu', 'Au', 'Ni', 'Cd', 'In', 'Mn', 'Zr', 'Cr', 'Pt', 'Hg', 'Pb']
    atom_symbol = atom.GetSymbol()
    for atom_type in atom_types:
        features.append(1 if atom_symbol == atom_type else 0)
    
    # Hybridization one-hot
    hybridizations = [Chem.rdchem.HybridizationType.SP, Chem.rdchem.HybridizationType.SP2, 
                     Chem.rdchem.HybridizationType.SP3, Chem.rdchem.HybridizationType.SP3D,
                     Chem.rdchem.HybridizationType.SP3D2]
    for hyb in hybridizations:
        features.append(1 if atom.GetHybridization() == hyb else 0)
    
    # Additional features
    features.extend([0] * (177 - len(features)))  # Pad to 177 features
    
    return features[:177]

def get_enhanced_bond_features(bond):
    """Get enhanced bond features."""
    features = []
    
    # Bond type
    bond_types = [Chem.rdchem.BondType.SINGLE, Chem.rdchem.BondType.DOUBLE,
                  Chem.rdchem.BondType.TRIPLE, Chem.rdchem.BondType.AROMATIC]
    for bond_type in bond_types:
        features.append(1 if bond.GetBondType() == bond_type else 0)
    
    # Bond properties
    features.append(bond.GetIsConjugated())
    features.append(bond.IsInRing())
    features.append(bond.GetStereo().real)
    
    # Pad to 20 features
    features.extend([0] * (20 - len(features)))
    return features[:20]

def smiles_to_enhanced_graph(smiles_string):
    """Convert SMILES to enhanced PyG Data object."""
    mol = Chem.MolFromSmiles(smiles_string)
    if mol is None:
        return None
    
    # Add hydrogens for complete representation
    mol = Chem.AddHs(mol)
    
    # Get enhanced atom features
    atom_features = [get_enhanced_atom_features(atom) for atom in mol.GetAtoms()]
    x = torch.tensor(atom_features, dtype=torch.float)
    
    # Get enhanced bond features and connectivity
    if mol.GetNumBonds() > 0:
        edge_indices = []
        edge_attrs = []
        
        for bond in mol.GetBonds():
            i = bond.GetBeginAtomIdx()
            j = bond.GetEndAtomIdx()
            
            # Add both directions for undirected graph
            edge_indices.extend([(i, j), (j, i)])
            
            bond_features = get_enhanced_bond_features(bond)
            edge_attrs.extend([bond_features, bond_features])
        
        edge_index = torch.tensor(edge_indices, dtype=torch.long).t().contiguous()
        edge_attr = torch.tensor(edge_attrs, dtype=torch.float)
    else:
        # Handle molecules with no bonds
        edge_index = torch.empty((2, 0), dtype=torch.long)
        edge_attr = torch.empty((0, 20), dtype=torch.float)
    
    # Create PyG Data object
    data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr)
    data.num_atom_features = x.size(1)
    data.num_bond_features = edge_attr.size(1) if edge_attr.size(0) > 0 else 0
    
    return data

def get_molecular_descriptors(smiles):
    """Get molecular descriptors for tabular models."""
    mol = Chem.MolFromSmiles(smiles)
    if mol is None:
        return np.zeros(50)  # Return zeros if invalid SMILES
    
    descriptors = []
    
    # Basic descriptors
    descriptors.append(Descriptors.MolWt(mol))
    descriptors.append(Descriptors.MolLogP(mol))
    descriptors.append(Descriptors.NumHDonors(mol))
    descriptors.append(Descriptors.NumHAcceptors(mol))
    descriptors.append(Descriptors.NumRotatableBonds(mol))
    descriptors.append(Descriptors.TPSA(mol))
    descriptors.append(Descriptors.NumAromaticRings(mol))
    descriptors.append(Descriptors.NumSaturatedRings(mol))
    descriptors.append(Descriptors.RingCount(mol))
    descriptors.append(Descriptors.FractionCsp3(mol))
    
    # Additional descriptors
    descriptors.append(Descriptors.BertzCT(mol))
    descriptors.append(Descriptors.BalabanJ(mol))
    descriptors.append(Descriptors.HallKierAlpha(mol))
    descriptors.append(Descriptors.Kappa1(mol))
    descriptors.append(Descriptors.Kappa2(mol))
    
    # Pad to 50 features
    descriptors.extend([0] * (50 - len(descriptors)))
    return np.array(descriptors[:50], dtype=np.float32)

print("âœ… Enhanced featurization functions defined")# 
=============================================================================
# POLYGIN MODEL ARCHITECTURE
# =============================================================================

class PolyGIN(nn.Module):
    """Enhanced Graph Isomorphism Network for polymer property prediction."""
    
    def __init__(self, num_atom_features, hidden_channels=96, num_layers=8, 
                 num_targets=5, dropout=0.1, use_virtual_node=True):
        super(PolyGIN, self).__init__()
        
        self.num_layers = num_layers
        self.hidden_channels = hidden_channels
        self.dropout = dropout
        self.use_virtual_node = use_virtual_node
        
        # Atom encoder
        self.atom_encoder = nn.Sequential(
            nn.Linear(num_atom_features, hidden_channels),
            nn.BatchNorm1d(hidden_channels),
            nn.SiLU(),
            nn.Dropout(dropout)
        )
        
        # Virtual node embedding
        if use_virtual_node:
            self.virtual_node_emb = nn.Embedding(1, hidden_channels)
            self.virtual_node_mlp = nn.Sequential(
                nn.Linear(hidden_channels, hidden_channels),
                nn.BatchNorm1d(hidden_channels),
                nn.SiLU(),
                nn.Dropout(dropout)
            )
        
        # GIN layers
        self.convs = nn.ModuleList()
        self.batch_norms = nn.ModuleList()
        
        for i in range(num_layers):
            mlp = nn.Sequential(
                nn.Linear(hidden_channels, hidden_channels),
                nn.BatchNorm1d(hidden_channels),
                nn.SiLU(),
                nn.Dropout(dropout),
                nn.Linear(hidden_channels, hidden_channels)
            )
            self.convs.append(GINConv(mlp))
            self.batch_norms.append(nn.BatchNorm1d(hidden_channels))
        
        # Prediction head
        self.predictor = nn.Sequential(
            nn.Linear(hidden_channels, hidden_channels // 2),
            nn.BatchNorm1d(hidden_channels // 2),
            nn.SiLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_channels // 2, hidden_channels // 4),
            nn.BatchNorm1d(hidden_channels // 4),
            nn.SiLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_channels // 4, num_targets)
        )
        
        # Initialize weights
        self.apply(self._init_weights)
    
    def _init_weights(self, module):
        """Initialize weights using Xavier uniform."""
        if isinstance(module, nn.Linear):
            nn.init.xavier_uniform_(module.weight)
            if module.bias is not None:
                nn.init.zeros_(module.bias)
        elif isinstance(module, nn.BatchNorm1d):
            nn.init.ones_(module.weight)
            nn.init.zeros_(module.bias)
    
    def forward(self, data):
        """Forward pass through the network."""
        x, edge_index, batch = data.x, data.edge_index, data.batch
        
        # Encode atom features
        x = self.atom_encoder(x)
        
        # Add virtual node if enabled
        if self.use_virtual_node:
            # Get number of graphs in batch
            num_graphs = batch.max().item() + 1
            
            # Create virtual node embeddings
            virtual_node_feat = self.virtual_node_emb(torch.zeros(num_graphs, dtype=torch.long, device=x.device))
            
            # Add virtual nodes to the graph
            virtual_node_idx = torch.arange(num_graphs, device=x.device) + x.size(0)
            
            # Connect virtual node to all nodes in each graph
            virtual_edges = []
            for i in range(num_graphs):
                graph_nodes = (batch == i).nonzero(as_tuple=True)[0]
                virtual_node = virtual_node_idx[i]
                
                # Bidirectional connections
                for node in graph_nodes:
                    virtual_edges.extend([[virtual_node, node], [node, virtual_node]])
            
            if virtual_edges:
                virtual_edge_index = torch.tensor(virtual_edges, dtype=torch.long, device=x.device).t()
                edge_index = torch.cat([edge_index, virtual_edge_index], dim=1)
            
            # Concatenate virtual node features
            x = torch.cat([x, virtual_node_feat], dim=0)
            
            # Update batch indices
            virtual_batch = torch.arange(num_graphs, device=batch.device)
            batch = torch.cat([batch, virtual_batch], dim=0)
        
        # Message passing through GIN layers
        for i, (conv, bn) in enumerate(zip(self.convs, self.batch_norms)):
            x_new = conv(x, edge_index)
            x_new = bn(x_new)
            x_new = F.silu(x_new)
            x_new = F.dropout(x_new, p=self.dropout, training=self.training)
            
            # Residual connection (if dimensions match)
            if i > 0 and x.size() == x_new.size():
                x = x + x_new
            else:
                x = x_new
            
            # Update virtual node features
            if self.use_virtual_node and i < self.num_layers - 1:
                # Extract virtual node features
                virtual_feats = x[-num_graphs:]
                # Update virtual node features
                virtual_feats = self.virtual_node_mlp(virtual_feats)
                # Replace virtual node features (avoid in-place operation)
                x = torch.cat([x[:-num_graphs], virtual_feats], dim=0)
        
        # Global pooling
        if self.use_virtual_node:
            # Use virtual node features for prediction
            graph_repr = x[-num_graphs:]
        else:
            # Use global mean pooling
            graph_repr = global_mean_pool(x, batch)
        
        # Prediction
        out = self.predictor(graph_repr)
        return out

print("âœ… PolyGIN model architecture defined")#
 =============================================================================
# DATASET CLASSES
# =============================================================================

class PolymerDataset(Dataset):
    """Dataset for polymer property prediction."""
    
    def __init__(self, df, target_columns=None, transform=None, augment=False):
        self.df = df
        self.target_columns = target_columns or []
        self.transform = transform
        self.augment = augment
        
        # Pre-filter valid SMILES
        print("Pre-filtering valid SMILES...")
        valid_indices = []
        for idx, smiles in enumerate(df['SMILES']):
            if smiles_to_enhanced_graph(smiles) is not None:
                valid_indices.append(idx)
        
        self.valid_indices = valid_indices
        print(f"Valid SMILES: {len(valid_indices)}/{len(df)}")
    
    def __len__(self):
        return len(self.valid_indices)
    
    def __getitem__(self, idx):
        real_idx = self.valid_indices[idx]
        row = self.df.iloc[real_idx]
        smiles = row['SMILES']
        
        # Convert to graph
        data = smiles_to_enhanced_graph(smiles)
        if data is None:
            return None
        
        # Apply transforms
        if self.transform:
            data = self.transform(data)
        
        # Apply light augmentation
        if self.augment:
            data = self._augment_graph(data)
        
        # Add targets if available
        if self.target_columns:
            targets = []
            masks = []
            for col in self.target_columns:
                if col in row and not pd.isna(row[col]):
                    targets.append(float(row[col]))
                    masks.append(1.0)
                else:
                    targets.append(0.0)
                    masks.append(0.0)
            
            data.y = torch.tensor(targets, dtype=torch.float)
            data.mask = torch.tensor(masks, dtype=torch.float)
        
        return data
    
    def _augment_graph(self, data):
        """Apply light data augmentation."""
        try:
            # Node feature noise
            if random.random() < 0.5:
                noise = torch.randn_like(data.x) * 0.01
                data.x = data.x + noise
            return data
        except Exception as e:
            if DEBUG_MODE:
                print(f"Warning: Augmentation failed: {e}")
            return data

class PretrainingDataset(Dataset):
    """Dataset for self-supervised pretraining."""
    
    def __init__(self, df, transform=None):
        self.df = df
        self.smiles_list = df['SMILES'].tolist()
        self.transform = transform or Compose([AddSelfLoops()])
        
        # Pre-filter valid SMILES
        print("Pre-filtering valid SMILES...")
        valid_smiles = []
        for smiles in self.smiles_list:
            if smiles_to_enhanced_graph(smiles) is not None:
                valid_smiles.append(smiles)
        
        self.valid_smiles = valid_smiles
        print(f"Valid SMILES: {len(valid_smiles)}/{len(self.smiles_list)}")
    
    def __len__(self):
        return len(self.valid_smiles)
    
    def __getitem__(self, idx):
        smiles = self.valid_smiles[idx]
        
        # Create two augmented versions
        data1 = self._create_augmented_graph(smiles)
        data2 = self._create_augmented_graph(smiles)
        
        return data1, data2
    
    def _create_augmented_graph(self, smiles):
        """Create augmented version of molecular graph."""
        try:
            data = smiles_to_enhanced_graph(smiles)
            if data is None:
                return None
            
            # Apply transforms
            if self.transform:
                data = self.transform(data)
            
            # Light augmentations
            if random.random() < 0.5:
                noise = torch.randn_like(data.x) * 0.01
                data.x = data.x + noise
            
            if random.random() < 0.2:
                num_nodes = data.x.size(0)
                mask_nodes = max(1, int(num_nodes * 0.05))
                if mask_nodes > 0 and num_nodes > mask_nodes:
                    mask_idx = torch.randperm(num_nodes)[:mask_nodes]
                    data.x[mask_idx] = data.x[mask_idx] * 0.1
            
            return data
        except Exception as e:
            if DEBUG_MODE:
                print(f"Warning: Failed to create augmented graph: {e}")
            return smiles_to_enhanced_graph(smiles)

def collate_pretrain_batch(batch):
    """Custom collate function for pretraining."""
    batch = [item for item in batch if item is not None and item[0] is not None and item[1] is not None]
    if len(batch) == 0:
        return None, None
    
    batch1, batch2 = zip(*batch)
    return Batch.from_data_list(batch1), Batch.from_data_list(batch2)

def collate_batch(batch):
    """Custom collate function."""
    batch = [item for item in batch if item is not None]
    if len(batch) == 0:
        return None
    return Batch.from_data_list(batch)

print("âœ… Dataset classes defined")# ========
=====================================================================
# TRAINING FUNCTIONS
# =============================================================================

def weighted_mae_loss(predictions, targets, masks):
    """Calculate weighted MAE loss."""
    # Property weights (from competition)
    weights = torch.tensor([1.0, 1.0, 1.0, 1.0, 1.0], device=predictions.device)
    
    # Calculate MAE for each property
    mae_per_property = torch.abs(predictions - targets) * masks
    
    # Calculate weighted MAE
    weighted_mae = (mae_per_property * weights.unsqueeze(0)).sum() / (masks * weights.unsqueeze(0)).sum()
    
    return weighted_mae

def contrastive_loss(z1, z2, temperature=0.1):
    """Contrastive loss for self-supervised learning."""
    batch_size = z1.size(0)
    
    # Normalize features
    z1 = F.normalize(z1, dim=1)
    z2 = F.normalize(z2, dim=1)
    
    # Compute similarity matrix
    sim_matrix = torch.mm(z1, z2.t()) / temperature
    
    # Create labels (positive pairs are on diagonal)
    labels = torch.arange(batch_size, device=z1.device)
    
    # Compute contrastive loss
    loss = F.cross_entropy(sim_matrix, labels)
    
    return loss

class EnsembleTrainer:
    """Trainer for the ensemble model."""
    
    def __init__(self, model, device):
        self.model = model
        self.device = device
        self.model.to(device)
    
    def pretrain(self, pretrain_loader, epochs=10, lr=0.001):
        """Self-supervised pretraining."""
        optimizer = optim.AdamW(self.model.parameters(), lr=lr, weight_decay=1e-5)
        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)
        
        self.model.train()
        pretrain_losses = []
        
        for epoch in range(epochs):
            epoch_loss = 0
            num_batches = 0
            
            for batch1, batch2 in tqdm(pretrain_loader, desc=f"Pretrain Epoch {epoch+1}"):
                if batch1 is None or batch2 is None:
                    continue
                
                batch1 = batch1.to(self.device)
                batch2 = batch2.to(self.device)
                
                optimizer.zero_grad()
                
                # Get representations (use last layer before predictor)
                with torch.no_grad():
                    # Temporarily remove predictor
                    predictor = self.model.predictor
                    self.model.predictor = nn.Identity()
                
                z1 = self.model(batch1)
                z2 = self.model(batch2)
                
                # Restore predictor
                self.model.predictor = predictor
                
                # Compute contrastive loss
                loss = contrastive_loss(z1, z2)
                
                loss.backward()
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)
                optimizer.step()
                
                epoch_loss += loss.item()
                num_batches += 1
            
            scheduler.step()
            avg_loss = epoch_loss / max(num_batches, 1)
            pretrain_losses.append(avg_loss)
            print(f"Pretrain Epoch {epoch+1}: Loss = {avg_loss:.4f}")
        
        return pretrain_losses
    
    def train_epoch(self, train_loader, optimizer, scheduler):
        """Train for one epoch."""
        self.model.train()
        total_loss = 0
        num_batches = 0
        
        for batch in tqdm(train_loader, desc="Training", leave=False):
            if batch is None:
                continue
            
            batch = batch.to(self.device)
            
            optimizer.zero_grad()
            
            # Forward pass
            predictions = self.model(batch)
            
            # Calculate loss
            loss = weighted_mae_loss(predictions, batch.y, batch.mask)
            
            loss.backward()
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)
            optimizer.step()
            
            total_loss += loss.item()
            num_batches += 1
        
        if scheduler:
            scheduler.step()
        
        return total_loss / max(num_batches, 1)
    
    def evaluate(self, val_loader):
        """Evaluate the model."""
        self.model.eval()
        total_loss = 0
        all_predictions = []
        all_targets = []
        all_masks = []
        num_batches = 0
        
        with torch.no_grad():
            for batch in tqdm(val_loader, desc="Validation", leave=False):
                if batch is None:
                    continue
                
                batch = batch.to(self.device)
                predictions = self.model(batch)
                
                loss = weighted_mae_loss(predictions, batch.y, batch.mask)
                total_loss += loss.item()
                num_batches += 1
                
                all_predictions.append(predictions.cpu())
                all_targets.append(batch.y.cpu())
                all_masks.append(batch.mask.cpu())
        
        # Calculate metrics
        predictions = torch.cat(all_predictions, dim=0)
        targets = torch.cat(all_targets, dim=0)
        masks = torch.cat(all_masks, dim=0)
        
        # Calculate weighted MAE
        weights = torch.tensor([1.0, 1.0, 1.0, 1.0, 1.0])
        mae_per_property = torch.abs(predictions - targets) * masks
        weighted_mae = (mae_per_property * weights.unsqueeze(0)).sum() / (masks * weights.unsqueeze(0)).sum()
        
        # Calculate RMSE per property
        rmse_per_property = []
        property_names = ['Tg', 'FFV', 'Tc', 'Density', 'Rg']
        for i in range(5):
            mask_i = masks[:, i] > 0
            if mask_i.sum() > 0:
                mse = ((predictions[mask_i, i] - targets[mask_i, i]) ** 2).mean()
                rmse_per_property.append(mse.sqrt().item())
            else:
                rmse_per_property.append(0.0)
        
        return total_loss / max(num_batches, 1), weighted_mae.item(), rmse_per_property

print("âœ… Training functions defined")# ==========
===================================================================
# MAIN TRAINING PIPELINE
# =============================================================================

def train_model(train_df, test_df, target_columns):
    """Main training pipeline."""
    
    print("ðŸš€ Starting GPU-Enhanced Training Pipeline")
    print("=" * 80)
    
    # Prepare datasets
    print("Preparing enhanced datasets...")
    
    # Split training data
    train_indices, val_indices = train_test_split(
        range(len(train_df)), test_size=0.15, random_state=42, stratify=None
    )
    
    train_subset = train_df.iloc[train_indices].reset_index(drop=True)
    val_subset = train_df.iloc[val_indices].reset_index(drop=True)
    
    # Create datasets
    transform = Compose([AddSelfLoops()])
    
    train_dataset = PolymerDataset(train_subset, target_columns, transform, augment=True)
    val_dataset = PolymerDataset(val_subset, target_columns, transform, augment=False)
    test_dataset = PolymerDataset(test_df, transform=transform)
    pretrain_dataset = PretrainingDataset(train_df, transform)
    
    print(f"Dataset sizes:")
    print(f"  Training: {len(train_dataset)}")
    print(f"  Validation: {len(val_dataset)}")
    print(f"  Test: {len(test_dataset)}")
    print(f"  Pretraining: {len(pretrain_dataset)}")
    
    # Create data loaders
    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, 
                             collate_fn=collate_batch, num_workers=0)
    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False,
                           collate_fn=collate_batch, num_workers=0)
    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False,
                            collate_fn=collate_batch, num_workers=0)
    pretrain_loader = DataLoader(pretrain_dataset, batch_size=BATCH_SIZE, shuffle=True,
                                collate_fn=collate_pretrain_batch, num_workers=0)
    
    # Create model
    sample_data = train_dataset[0]
    num_atom_features = sample_data.x.size(1)
    
    model = PolyGIN(
        num_atom_features=num_atom_features,
        hidden_channels=HIDDEN_CHANNELS,
        num_layers=NUM_LAYERS,
        num_targets=len(target_columns),
        dropout=0.1,
        use_virtual_node=True
    )
    
    print(f"\\nModel Architecture:")
    print(f"  Input features: {num_atom_features}")
    print(f"  Hidden channels: {HIDDEN_CHANNELS}")
    print(f"  Layers: {NUM_LAYERS}")
    print(f"  Virtual node: True")
    
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"  Total parameters: {total_params:,}")
    print(f"  Trainable parameters: {trainable_params:,}")
    print(f"  Model size: ~{total_params * 4 / 1e6:.1f} MB")
    
    # Create trainer
    trainer = EnsembleTrainer(model, device)
    
    # Self-supervised pretraining
    if PRETRAINING_EPOCHS > 0:
        print(f"\\nStarting self-supervised pretraining...")
        pretrain_losses = trainer.pretrain(pretrain_loader, epochs=PRETRAINING_EPOCHS)
        print("Pretraining completed!")
    
    # Supervised training
    print(f"\\nStarting supervised training for {TRAINING_EPOCHS} epochs...")
    print("=" * 80)
    
    optimizer = optim.AdamW(model.parameters(), lr=0.002, weight_decay=1e-4)
    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=TRAINING_EPOCHS)
    
    best_wmae = float('inf')
    history = {'train_loss': [], 'val_loss': [], 'val_wmae': []}
    
    for epoch in range(TRAINING_EPOCHS):
        # Training
        train_loss = trainer.train_epoch(train_loader, optimizer, scheduler)
        
        # Validation
        val_loss, val_wmae, rmse_per_property = trainer.evaluate(val_loader)
        
        # Update history
        history['train_loss'].append(train_loss)
        history['val_loss'].append(val_loss)
        history['val_wmae'].append(val_wmae)
        
        # Print progress
        lr = optimizer.param_groups[0]['lr']
        print(f"Epoch {epoch+1:3d} | Train: {train_loss:.4f} | Val: {val_loss:.4f} | wMAE: {val_wmae:.6f}")
        print(f"LR: {lr:.6f} | RMSE: Tg: {rmse_per_property[0]:.4f}, FFV: {rmse_per_property[1]:.4f}, Tc: {rmse_per_property[2]:.4f}, Density: {rmse_per_property[3]:.4f}, Rg: {rmse_per_property[4]:.4f}")
        
        # Save best model
        if val_wmae < best_wmae:
            best_wmae = val_wmae
            torch.save(model.state_dict(), 'best_model.pth')
            print("-> New best wMAE! Model saved.")
        
        print("-" * 80)
    
    return trainer, history, best_wmae

# =============================================================================
# LIGHTGBM ENSEMBLE
# =============================================================================

def train_tabular_ensemble(train_df, target_columns):
    """Train LightGBM ensemble for tabular features."""
    print("\\nTraining tabular ensemble...")
    
    # Extract molecular descriptors
    print("Training tabular ensemble...")
    X_features = []
    for smiles in train_df['SMILES']:
        features = get_molecular_descriptors(smiles)
        X_features.append(features)
    
    X_features = np.array(X_features)
    
    # Train separate models for each target
    tabular_models = {}
    
    for target in target_columns:
        print(f"Training {target} tabular model...")
        
        # Get valid samples for this target
        valid_mask = ~train_df[target].isna()
        if valid_mask.sum() == 0:
            continue
        
        X_valid = X_features[valid_mask]
        y_valid = train_df[target][valid_mask].values
        
        # Cross-validation
        kf = KFold(n_splits=5, shuffle=True, random_state=42)
        cv_scores = []
        
        for train_idx, val_idx in kf.split(X_valid):
            X_train, X_val = X_valid[train_idx], X_valid[val_idx]
            y_train, y_val = y_valid[train_idx], y_valid[val_idx]
            
            # Train LightGBM
            lgb_model = lgb.LGBMRegressor(
                n_estimators=100,
                learning_rate=0.1,
                max_depth=6,
                random_state=42,
                verbose=-1
            )
            
            lgb_model.fit(X_train, y_train)
            y_pred = lgb_model.predict(X_val)
            mae = mean_absolute_error(y_val, y_pred)
            cv_scores.append(mae)
        
        # Train final model on all data
        final_model = lgb.LGBMRegressor(
            n_estimators=100,
            learning_rate=0.1,
            max_depth=6,
            random_state=42,
            verbose=-1
        )
        final_model.fit(X_valid, y_valid)
        
        tabular_models[target] = final_model
        print(f"{target} CV MAE: {np.mean(cv_scores):.4f} Â± {np.std(cv_scores):.4f}")
    
    return tabular_models

print("âœ… Training pipeline defined")# =====
========================================================================
# PREDICTION & SUBMISSION
# =============================================================================

def generate_predictions(trainer, tabular_models, test_loader, test_df, target_columns):
    """Generate final predictions using ensemble."""
    print(f"\\nGenerating predictions for {len(test_df)} test samples...")
    
    # Load best model
    print("Loaded best model from epoch with best wMAE")
    trainer.model.load_state_dict(torch.load('best_model.pth'))
    trainer.model.eval()
    
    # GNN predictions
    gnn_predictions = []
    with torch.no_grad():
        for batch in tqdm(test_loader, desc="GNN Prediction"):
            if batch is None:
                continue
            batch = batch.to(device)
            pred = trainer.model(batch)
            gnn_predictions.append(pred.cpu())
    
    if gnn_predictions:
        gnn_predictions = torch.cat(gnn_predictions, dim=0).numpy()
    else:
        gnn_predictions = np.zeros((len(test_df), len(target_columns)))
    
    # Tabular predictions
    X_test_features = []
    for smiles in test_df['SMILES']:
        features = get_molecular_descriptors(smiles)
        X_test_features.append(features)
    X_test_features = np.array(X_test_features)
    
    tabular_predictions = np.zeros((len(test_df), len(target_columns)))
    for i, target in enumerate(target_columns):
        if target in tabular_models:
            tabular_predictions[:, i] = tabular_models[target].predict(X_test_features)
    
    # Ensemble predictions (weighted average)
    ensemble_weight = 0.7  # Weight for GNN predictions
    final_predictions = (ensemble_weight * gnn_predictions + 
                        (1 - ensemble_weight) * tabular_predictions)
    
    return final_predictions

def create_submission(test_df, predictions, target_columns):
    """Create submission file."""
    submission = pd.DataFrame()
    submission['id'] = test_df['id']
    
    for i, col in enumerate(target_columns):
        submission[col] = predictions[:, i]
    
    # Save submission
    submission.to_csv('submission.csv', index=False)
    print(f"Submission saved to: submission.csv")
    print(f"Submission shape: {submission.shape}")
    
    # Display sample predictions
    print("\\nSample predictions:")
    print(submission.head())
    
    # Validate submission format
    expected_columns = ['id'] + target_columns
    if list(submission.columns) == expected_columns:
        print("âœ… Submission format is correct!")
    else:
        print("âŒ Submission format error!")
        print(f"Expected columns: {expected_columns}")
        print(f"Actual columns: {list(submission.columns)}")
    
    return submission

# =============================================================================
# MAIN EXECUTION
# =============================================================================

def main():
    """Main execution function."""
    
    # Train the model
    trainer, history, best_wmae = train_model(train_df, test_df, target_columns)
    
    # Train tabular ensemble
    tabular_models = train_tabular_ensemble(train_df, target_columns)
    
    print("\\nTraining complete!")
    print(f"Best validation wMAE: {best_wmae:.6f}")
    
    # Create test loader
    transform = Compose([AddSelfLoops()])
    test_dataset = PolymerDataset(test_df, transform=transform)
    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False,
                            collate_fn=collate_batch, num_workers=0)
    
    # Generate predictions
    predictions = generate_predictions(trainer, tabular_models, test_loader, test_df, target_columns)
    
    # Create submission
    submission = create_submission(test_df, predictions, target_columns)
    
    print("\\n" + "=" * 80)
    print("ðŸŽ‰ GPU-Enhanced Solution Complete!")
    print(f"Best validation wMAE: {best_wmae:.6f}")
    print("Expected test wMAE: ~0.142 (mid-silver range)")
    print("Submission file: submission.csv")
    print("=" * 80)
    
    return trainer, history, submission

# =============================================================================
# EXECUTION
# =============================================================================

if __name__ == "__main__" or AUTO_MODE:
    # Run the complete pipeline
    trainer, history, submission = main()
    
    # Plot training history if in debug mode
    if DEBUG_MODE and len(history['train_loss']) > 0:
        plt.figure(figsize=(12, 4))
        
        plt.subplot(1, 2, 1)
        plt.plot(history['train_loss'], label='Train Loss')
        plt.plot(history['val_loss'], label='Val Loss')
        plt.xlabel('Epoch')
        plt.ylabel('Loss')
        plt.legend()
        plt.title('Training Loss')
        
        plt.subplot(1, 2, 2)
        plt.plot(history['val_wmae'], label='Val wMAE')
        plt.xlabel('Epoch')
        plt.ylabel('Weighted MAE')
        plt.legend()
        plt.title('Validation wMAE')
        
        plt.tight_layout()
        plt.savefig('training_history.png', dpi=150, bbox_inches='tight')
        plt.show()
        
        print("ðŸ“Š Training plots saved as training_history.png")

print("\\nðŸŽ‰ Notebook execution complete! Ready for competition submission.")