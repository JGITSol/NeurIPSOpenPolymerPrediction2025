{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# NeurIPS Open Polymer Prediction 2025 - Complete Solution\n",
                "\n",
                "This notebook contains the complete solution for the NeurIPS Open Polymer Prediction 2025 challenge.\n",
                "\n",
                "## Challenge Overview\n",
                "\n",
                "- **Task**: Predict 5 polymer properties from SMILES strings\n",
                "- **Properties**: Tg (glass transition temperature), FFV (fractional free volume), Tc (thermal conductivity), Density, Rg (radius of gyration)\n",
                "- **Evaluation**: Weighted Mean Absolute Error (wMAE)\n",
                "- **Data**: 7,973 training samples with significant missing values\n",
                "\n",
                "## Table of Contents\n",
                "1. [Setup and Imports](#setup)\n",
                "2. [Data Loading and Exploration](#data)\n",
                "3. [Molecular Featurization](#featurization)\n",
                "4. [Dataset Implementation](#dataset)\n",
                "5. [Model Architecture](#model)\n",
                "6. [Training Implementation](#training)\n",
                "7. [Competition Metrics](#metrics)\n",
                "8. [Training Loop](#training-loop)\n",
                "9. [Evaluation and Submission](#submission)\n",
                "10. [Results and Analysis](#results)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup and Imports\n",
                "\n",
                "First, let's install the required packages and import all necessary libraries."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install required packages (run this if packages are not installed)\n",
                "# !pip install torch torch-geometric rdkit pandas numpy scikit-learn tqdm matplotlib seaborn"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Core imports\n",
                "import os\n",
                "import sys\n",
                "import random\n",
                "import warnings\n",
                "from datetime import datetime\n",
                "from typing import List, Dict, Tuple, Optional\n",
                "\n",
                "# Data handling\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "from tqdm import tqdm\n",
                "\n",
                "# Visualization\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "\n",
                "# PyTorch and PyTorch Geometric\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "from torch.utils.data import random_split\n",
                "from torch_geometric.data import Data, Dataset, DataLoader\n",
                "from torch_geometric.nn import GCNConv, global_mean_pool\n",
                "\n",
                "# RDKit for chemistry\n",
                "from rdkit import Chem\n",
                "from rdkit.Chem import rdchem\n",
                "from rdkit import RDLogger\n",
                "\n",
                "# Suppress RDKit warnings\n",
                "RDLogger.DisableLog('rdApp.*')\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "# Set style for plots\n",
                "plt.style.use('default')\n",
                "sns.set_palette(\"husl\")\n",
                "\n",
                "print(\"All imports successful!\")\n",
                "print(f\"PyTorch version: {torch.__version__}\")\n",
                "print(f\"Device available: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Configuration\n",
                "class Config:\n",
                "    \"\"\"Configuration class for model hyperparameters and settings.\"\"\"\n",
                "    \n",
                "    def __init__(self):\n",
                "        # Device\n",
                "        self.DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "        \n",
                "        # Model hyperparameters\n",
                "        self.NUM_EPOCHS = 50\n",
                "        self.BATCH_SIZE = 32\n",
                "        self.LEARNING_RATE = 1e-3\n",
                "        self.WEIGHT_DECAY = 1e-5\n",
                "        self.HIDDEN_CHANNELS = 128\n",
                "        self.NUM_GCN_LAYERS = 3\n",
                "        \n",
                "        # Data\n",
                "        self.VAL_SPLIT_FRACTION = 0.2\n",
                "        self.SEED = 42\n",
                "        \n",
                "        # Target properties\n",
                "        self.TARGET_PROPERTIES = ['Tg', 'FFV', 'Tc', 'Density', 'Rg']\n",
                "\n",
                "# Create configuration instance\n",
                "CONFIG = Config()\n",
                "\n",
                "print(f\"Configuration:\")\n",
                "print(f\"  Device: {CONFIG.DEVICE}\")\n",
                "print(f\"  Batch size: {CONFIG.BATCH_SIZE}\")\n",
                "print(f\"  Learning rate: {CONFIG.LEARNING_RATE}\")\n",
                "print(f\"  Hidden channels: {CONFIG.HIDDEN_CHANNELS}\")\n",
                "print(f\"  Target properties: {CONFIG.TARGET_PROPERTIES}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Set random seeds for reproducibility\n",
                "def set_seed(seed: int = 42):\n",
                "    \"\"\"Set random seeds for reproducibility.\"\"\"\n",
                "    random.seed(seed)\n",
                "    np.random.seed(seed)\n",
                "    torch.manual_seed(seed)\n",
                "    if torch.cuda.is_available():\n",
                "        torch.cuda.manual_seed(seed)\n",
                "        torch.cuda.manual_seed_all(seed)\n",
                "        torch.backends.cudnn.deterministic = True\n",
                "        torch.backends.cudnn.benchmark = False\n",
                "\n",
                "set_seed(CONFIG.SEED)\n",
                "print(f\"Random seed set to {CONFIG.SEED}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Data Loading and Exploration\n",
                "\n",
                "Let's load the competition data and explore its structure."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load competition data\n",
                "def load_competition_data():\n",
                "    \"\"\"Load the competition data files.\"\"\"\n",
                "    train_path = \"info/train.csv\"\n",
                "    test_path = \"info/test.csv\"\n",
                "    \n",
                "    if not os.path.exists(train_path):\n",
                "        raise FileNotFoundError(f\"Training data not found at {train_path}\")\n",
                "    if not os.path.exists(test_path):\n",
                "        raise FileNotFoundError(f\"Test data not found at {test_path}\")\n",
                "    \n",
                "    train_df = pd.read_csv(train_path)\n",
                "    test_df = pd.read_csv(test_path)\n",
                "    \n",
                "    print(f\"Loaded training data: {len(train_df)} samples\")\n",
                "    print(f\"Loaded test data: {len(test_df)} samples\")\n",
                "    \n",
                "    return train_df, test_df\n",
                "\n",
                "# Load data\n",
                "train_df, test_df = load_competition_data()\n",
                "\n",
                "# Display basic info\n",
                "print(\"\\nTraining data columns:\", train_df.columns.tolist())\n",
                "print(\"Training data shape:\", train_df.shape)\n",
                "print(\"Test data shape:\", test_df.shape)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Explore the data\n",
                "print(\"First few rows of training data:\")\n",
                "display(train_df.head())\n",
                "\n",
                "print(\"\\nFirst few rows of test data:\")\n",
                "display(test_df.head())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Analyze missing values\n",
                "print(\"Missing values analysis:\")\n",
                "print(\"=\" * 50)\n",
                "\n",
                "missing_stats = []\n",
                "for col in CONFIG.TARGET_PROPERTIES:\n",
                "    if col in train_df.columns:\n",
                "        missing = train_df[col].isna().sum()\n",
                "        total = len(train_df)\n",
                "        percentage = (missing / total) * 100\n",
                "        missing_stats.append({\n",
                "            'Property': col,\n",
                "            'Missing': missing,\n",
                "            'Total': total,\n",
                "            'Percentage': percentage\n",
                "        })\n",
                "        print(f\"{col:>8}: {missing:>5}/{total} ({percentage:>5.1f}%)\")\n",
                "\n",
                "# Create a DataFrame for visualization\n",
                "missing_df = pd.DataFrame(missing_stats)\n",
                "\n",
                "# Visualize missing values\n",
                "plt.figure(figsize=(12, 5))\n",
                "plt.subplot(1, 2, 1)\n",
                "sns.barplot(data=missing_df, x='Property', y='Percentage')\n",
                "plt.title('Missing Values by Property')\n",
                "plt.ylabel('Missing Percentage (%)')\n",
                "plt.xticks(rotation=45)\n",
                "\n",
                "plt.subplot(1, 2, 2)\n",
                "# Heatmap of missing values pattern\n",
                "missing_matrix = train_df[CONFIG.TARGET_PROPERTIES].isna().astype(int)\n",
                "sns.heatmap(missing_matrix.iloc[:100], cmap='RdYlBu_r', cbar_kws={'label': 'Missing (1) / Present (0)'})\n",
                "plt.title('Missing Values Pattern (First 100 samples)')\n",
                "plt.xlabel('Properties')\n",
                "plt.ylabel('Samples')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Analyze property distributions\n",
                "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
                "axes = axes.flatten()\n",
                "\n",
                "for i, prop in enumerate(CONFIG.TARGET_PROPERTIES):\n",
                "    if prop in train_df.columns:\n",
                "        # Remove missing values for plotting\n",
                "        values = train_df[prop].dropna()\n",
                "        \n",
                "        axes[i].hist(values, bins=50, alpha=0.7, edgecolor='black')\n",
                "        axes[i].set_title(f'{prop} Distribution\\n(n={len(values)}, mean={values.mean():.3f})')\n",
                "        axes[i].set_xlabel(prop)\n",
                "        axes[i].set_ylabel('Frequency')\n",
                "        axes[i].grid(True, alpha=0.3)\n",
                "\n",
                "# Remove empty subplot\n",
                "fig.delaxes(axes[5])\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "# Summary statistics\n",
                "print(\"\\nSummary statistics for target properties:\")\n",
                "display(train_df[CONFIG.TARGET_PROPERTIES].describe())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Molecular Featurization\n",
                "\n",
                "Convert SMILES strings to graph representations using RDKit."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def get_atom_features(atom):\n",
                "    \"\"\"Extract features for a single atom.\"\"\"\n",
                "    # Basic atom features\n",
                "    features = [\n",
                "        atom.GetAtomicNum(),\n",
                "        atom.GetDegree(),\n",
                "        atom.GetTotalNumHs(),\n",
                "        atom.GetTotalValence(),\n",
                "        int(atom.GetIsAromatic()),\n",
                "        int(atom.GetChiralTag()),\n",
                "        atom.GetFormalCharge(),\n",
                "        int(atom.IsInRing()),\n",
                "    ]\n",
                "    \n",
                "    # One-hot encode common atomic numbers in polymers\n",
                "    common_atoms = [1, 6, 7, 8, 9, 14, 15, 16, 17, 35, 53]  # H, C, N, O, F, Si, P, S, Cl, Br, I\n",
                "    atomic_num_one_hot = [0] * (len(common_atoms) + 1)  # +1 for \"other\"\n",
                "    \n",
                "    atomic_num = atom.GetAtomicNum()\n",
                "    if atomic_num in common_atoms:\n",
                "        atomic_num_one_hot[common_atoms.index(atomic_num)] = 1\n",
                "    else:\n",
                "        atomic_num_one_hot[-1] = 1  # \"other\" category\n",
                "    \n",
                "    # Hybridization one-hot encoding\n",
                "    hybridization_types = [\n",
                "        rdchem.HybridizationType.SP,\n",
                "        rdchem.HybridizationType.SP2,\n",
                "        rdchem.HybridizationType.SP3,\n",
                "        rdchem.HybridizationType.SP3D,\n",
                "        rdchem.HybridizationType.SP3D2,\n",
                "    ]\n",
                "    hybridization_one_hot = [0] * (len(hybridization_types) + 1)  # +1 for \"other\"\n",
                "    \n",
                "    hybridization = atom.GetHybridization()\n",
                "    if hybridization in hybridization_types:\n",
                "        hybridization_one_hot[hybridization_types.index(hybridization)] = 1\n",
                "    else:\n",
                "        hybridization_one_hot[-1] = 1\n",
                "    \n",
                "    return features + atomic_num_one_hot + hybridization_one_hot\n",
                "\n",
                "\n",
                "def get_bond_features(bond):\n",
                "    \"\"\"Extract features for a single bond.\"\"\"\n",
                "    # Bond type one-hot encoding\n",
                "    bond_types = [\n",
                "        Chem.rdchem.BondType.SINGLE,\n",
                "        Chem.rdchem.BondType.DOUBLE,\n",
                "        Chem.rdchem.BondType.TRIPLE,\n",
                "        Chem.rdchem.BondType.AROMATIC,\n",
                "    ]\n",
                "    bond_type_one_hot = [0] * (len(bond_types) + 1)  # +1 for \"other\"\n",
                "    \n",
                "    bond_type = bond.GetBondType()\n",
                "    if bond_type in bond_types:\n",
                "        bond_type_one_hot[bond_types.index(bond_type)] = 1\n",
                "    else:\n",
                "        bond_type_one_hot[-1] = 1\n",
                "    \n",
                "    # Additional bond features\n",
                "    features = [\n",
                "        int(bond.IsInRing()),\n",
                "        int(bond.GetIsConjugated()),\n",
                "    ]\n",
                "    \n",
                "    return features + bond_type_one_hot\n",
                "\n",
                "\n",
                "def smiles_to_graph(smiles_string: str):\n",
                "    \"\"\"Convert a SMILES string to a PyG Data object.\"\"\"\n",
                "    mol = Chem.MolFromSmiles(smiles_string)\n",
                "    if mol is None:\n",
                "        return None\n",
                "    \n",
                "    mol = Chem.AddHs(mol)  # Add explicit hydrogens\n",
                "\n",
                "    # Get atom features\n",
                "    atom_features = [get_atom_features(atom) for atom in mol.GetAtoms()]\n",
                "    x = torch.tensor(atom_features, dtype=torch.float)\n",
                "\n",
                "    # Get bond features and connectivity\n",
                "    if mol.GetNumBonds() > 0:\n",
                "        edge_indices = []\n",
                "        edge_attrs = []\n",
                "        for bond in mol.GetBonds():\n",
                "            i = bond.GetBeginAtomIdx()\n",
                "            j = bond.GetEndAtomIdx()\n",
                "            edge_indices.append((i, j))\n",
                "            edge_indices.append((j, i))  # Graph must be undirected\n",
                "\n",
                "            # Enhanced bond features\n",
                "            bond_features = get_bond_features(bond)\n",
                "            edge_attrs.append(bond_features)\n",
                "            edge_attrs.append(bond_features)  # Same features for both directions\n",
                "\n",
                "        edge_index = torch.tensor(edge_indices, dtype=torch.long).t().contiguous()\n",
                "        edge_attr = torch.tensor(edge_attrs, dtype=torch.float)\n",
                "    else:\n",
                "        # Handle molecules with no bonds (e.g., single atoms)\n",
                "        edge_index = torch.empty((2, 0), dtype=torch.long)\n",
                "        edge_attr = torch.empty((0, 7), dtype=torch.float)  # 7 bond features\n",
                "\n",
                "    # Create the PyG Data object\n",
                "    data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr)\n",
                "    \n",
                "    # Store number of features for model instantiation later\n",
                "    data.num_atom_features = x.size(1)\n",
                "\n",
                "    return data\n",
                "\n",
                "print(\"Molecular featurization functions defined!\")\n",
                "print(f\"Expected atom feature size: {len(get_atom_features(Chem.MolFromSmiles('C').GetAtomWithIdx(0)))}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test featurization on a sample SMILES\n",
                "sample_smiles = train_df['SMILES'].iloc[0]\n",
                "print(f\"Sample SMILES: {sample_smiles}\")\n",
                "\n",
                "sample_graph = smiles_to_graph(sample_smiles)\n",
                "if sample_graph is not None:\n",
                "    print(f\"Graph created successfully!\")\n",
                "    print(f\"  Number of atoms: {sample_graph.x.size(0)}\")\n",
                "    print(f\"  Atom feature size: {sample_graph.x.size(1)}\")\n",
                "    print(f\"  Number of bonds: {sample_graph.edge_index.size(1) // 2}\")\n",
                "    print(f\"  Edge feature size: {sample_graph.edge_attr.size(1) if sample_graph.edge_attr.size(0) > 0 else 0}\")\n",
                "else:\n",
                "    print(\"Failed to create graph from SMILES\")\n",
                "\n",
                "# Test on a few more samples to check for failures\n",
                "print(\"\\nTesting featurization on first 10 SMILES...\")\n",
                "success_count = 0\n",
                "for i, smiles in enumerate(train_df['SMILES'].head(10)):\n",
                "    graph = smiles_to_graph(smiles)\n",
                "    if graph is not None:\n",
                "        success_count += 1\n",
                "    else:\n",
                "        print(f\"Failed on sample {i}: {smiles}\")\n",
                "\n",
                "print(f\"Success rate: {success_count}/10\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Dataset Implementation\n",
                "\n",
                "Create a PyTorch Geometric dataset that handles multi-target prediction and missing values."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class PolymerDataset(Dataset):\n",
                "    \"\"\"PyTorch Geometric Dataset for polymer data with multi-target prediction.\"\"\"\n",
                "\n",
                "    def __init__(self, df: pd.DataFrame, target_cols: Optional[List[str]] = None, is_test: bool = False):\n",
                "        super().__init__()\n",
                "        self.df = df\n",
                "        self.is_test = is_test\n",
                "        \n",
                "        # Default target columns for the competition\n",
                "        if target_cols is None:\n",
                "            target_cols = CONFIG.TARGET_PROPERTIES\n",
                "        self.target_cols = target_cols\n",
                "        \n",
                "        self.smiles_list = df['SMILES'].tolist()\n",
                "        self.ids = df['id'].tolist()\n",
                "        \n",
                "        if not is_test:\n",
                "            # Extract targets and create masks for missing values\n",
                "            self.targets = []\n",
                "            self.masks = []\n",
                "            \n",
                "            for idx in range(len(df)):\n",
                "                target_values = []\n",
                "                mask_values = []\n",
                "                \n",
                "                for col in target_cols:\n",
                "                    if col in df.columns:\n",
                "                        val = df.iloc[idx][col]\n",
                "                        if pd.isna(val):\n",
                "                            target_values.append(0.0)  # Placeholder for missing values\n",
                "                            mask_values.append(0.0)    # Mask indicates missing\n",
                "                        else:\n",
                "                            target_values.append(float(val))\n",
                "                            mask_values.append(1.0)    # Mask indicates present\n",
                "                    else:\n",
                "                        target_values.append(0.0)\n",
                "                        mask_values.append(0.0)\n",
                "                \n",
                "                self.targets.append(target_values)\n",
                "                self.masks.append(mask_values)\n",
                "        \n",
                "        self.cache = {}  # Cache graphs to avoid re-computing\n",
                "\n",
                "    def len(self):\n",
                "        return len(self.df)\n",
                "\n",
                "    def get(self, idx):\n",
                "        if idx in self.cache:\n",
                "            data = self.cache[idx]\n",
                "        else:\n",
                "            smiles = self.smiles_list[idx]\n",
                "            data = smiles_to_graph(smiles)\n",
                "            if data is None:  # Handle RDKit parsing errors\n",
                "                return None\n",
                "            self.cache[idx] = data\n",
                "\n",
                "        # Add ID (store as integer, not tensor)\n",
                "        data.id = int(self.ids[idx])\n",
                "        \n",
                "        if not self.is_test:\n",
                "            # Add target values and masks - reshape to (1, 5) to preserve structure during batching\n",
                "            data.y = torch.tensor(self.targets[idx], dtype=torch.float).unsqueeze(0)  # Shape: (1, 5)\n",
                "            data.mask = torch.tensor(self.masks[idx], dtype=torch.float).unsqueeze(0)  # Shape: (1, 5)\n",
                "        \n",
                "        return data\n",
                "\n",
                "print(\"PolymerDataset class defined!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test the dataset\n",
                "print(\"Testing dataset creation...\")\n",
                "\n",
                "# Create a small test dataset\n",
                "test_dataset = PolymerDataset(train_df.head(10), is_test=False)\n",
                "print(f\"Dataset length: {len(test_dataset)}\")\n",
                "\n",
                "# Test a single sample\n",
                "sample = test_dataset.get(0)\n",
                "if sample is not None:\n",
                "    print(f\"Sample successful:\")\n",
                "    print(f\"  ID: {sample.id}\")\n",
                "    print(f\"  Node features shape: {sample.x.shape}\")\n",
                "    print(f\"  Edge index shape: {sample.edge_index.shape}\")\n",
                "    print(f\"  Targets shape: {sample.y.shape}\")\n",
                "    print(f\"  Mask shape: {sample.mask.shape}\")\n",
                "    print(f\"  Targets: {sample.y.squeeze()}\")\n",
                "    print(f\"  Mask: {sample.mask.squeeze()}\")\n",
                "else:\n",
                "    print(\"Sample failed\")\n",
                "\n",
                "# Test batching\n",
                "print(\"\\nTesting data loader...\")\n",
                "loader = DataLoader(test_dataset, batch_size=4, shuffle=False)\n",
                "for batch in loader:\n",
                "    print(f\"Batch:\")\n",
                "    print(f\"  Batch size: {batch.num_graphs}\")\n",
                "    print(f\"  Node features shape: {batch.x.shape}\")\n",
                "    print(f\"  Targets shape: {batch.y.shape}\")\n",
                "    print(f\"  Mask shape: {batch.mask.shape}\")\n",
                "    break"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Model Architecture\n",
                "\n",
                "Implement a Graph Convolutional Network for multi-target polymer property prediction."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class PolymerGCN(nn.Module):\n",
                "    \"\"\"Graph Convolutional Network for polymer property prediction.\"\"\"\n",
                "\n",
                "    def __init__(self, num_atom_features: int, hidden_channels: int, num_gcn_layers: int):\n",
                "        super().__init__()\n",
                "        self.convs = nn.ModuleList()\n",
                "        self.bns = nn.ModuleList()\n",
                "\n",
                "        # Input layer\n",
                "        self.convs.append(GCNConv(num_atom_features, hidden_channels))\n",
                "        self.bns.append(nn.BatchNorm1d(hidden_channels))\n",
                "\n",
                "        # Hidden layers\n",
                "        for _ in range(num_gcn_layers - 1):\n",
                "            self.convs.append(GCNConv(hidden_channels, hidden_channels))\n",
                "            self.bns.append(nn.BatchNorm1d(hidden_channels))\n",
                "\n",
                "        # Output layer for multi-target prediction (5 properties)\n",
                "        self.out = nn.Sequential(\n",
                "            nn.Linear(hidden_channels, hidden_channels // 2),\n",
                "            nn.ReLU(),\n",
                "            nn.Linear(hidden_channels // 2, 5)  # 5 properties: Tg, FFV, Tc, Density, Rg\n",
                "        )\n",
                "\n",
                "    def forward(self, data):\n",
                "        \"\"\"Forward pass through the network.\"\"\"\n",
                "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
                "        \n",
                "        # Message passing through GCN layers\n",
                "        for conv, bn in zip(self.convs, self.bns):\n",
                "            x = conv(x, edge_index)\n",
                "            x = bn(x)\n",
                "            x = F.relu(x)\n",
                "\n",
                "        # Readout phase: Aggregate node features into a single graph-level representation\n",
                "        x = global_mean_pool(x, batch)\n",
                "        \n",
                "        # Final prediction\n",
                "        return self.out(x)\n",
                "\n",
                "print(\"PolymerGCN model class defined!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test model creation\n",
                "print(\"Testing model creation...\")\n",
                "\n",
                "# Get feature dimensions from a sample\n",
                "sample_data = test_dataset.get(0)\n",
                "num_atom_features = sample_data.num_atom_features\n",
                "\n",
                "# Create model\n",
                "model = PolymerGCN(\n",
                "    num_atom_features=num_atom_features,\n",
                "    hidden_channels=CONFIG.HIDDEN_CHANNELS,\n",
                "    num_gcn_layers=CONFIG.NUM_GCN_LAYERS\n",
                ").to(CONFIG.DEVICE)\n",
                "\n",
                "print(f\"Model created successfully!\")\n",
                "print(f\"  Input features: {num_atom_features}\")\n",
                "print(f\"  Hidden channels: {CONFIG.HIDDEN_CHANNELS}\")\n",
                "print(f\"  GCN layers: {CONFIG.NUM_GCN_LAYERS}\")\n",
                "print(f\"  Output targets: 5\")\n",
                "print(f\"  Device: {CONFIG.DEVICE}\")\n",
                "\n",
                "# Count parameters\n",
                "total_params = sum(p.numel() for p in model.parameters())\n",
                "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
                "print(f\"  Total parameters: {total_params:,}\")\n",
                "print(f\"  Trainable parameters: {trainable_params:,}\")\n",
                "\n",
                "# Test forward pass\n",
                "model.eval()\n",
                "with torch.no_grad():\n",
                "    # Create a small batch\n",
                "    loader = DataLoader(test_dataset, batch_size=2, shuffle=False)\n",
                "    batch = next(iter(loader))\n",
                "    batch = batch.to(CONFIG.DEVICE)\n",
                "    \n",
                "    output = model(batch)\n",
                "    print(f\"\\nForward pass test:\")\n",
                "    print(f\"  Input batch size: {batch.num_graphs}\")\n",
                "    print(f\"  Output shape: {output.shape}\")\n",
                "    print(f\"  Output sample: {output[0].cpu().numpy()}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Training Implementation\n",
                "\n",
                "Implement training functions that handle missing values and multi-target prediction."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def masked_mse_loss(predictions, targets, masks):\n",
                "    \"\"\"Calculate MSE loss only for non-missing values.\"\"\"\n",
                "    # Ensure all tensors have the same shape\n",
                "    assert predictions.shape == targets.shape == masks.shape, f\"Shape mismatch: pred {predictions.shape}, target {targets.shape}, mask {masks.shape}\"\n",
                "    \n",
                "    # Only compute loss for non-missing values\n",
                "    masked_predictions = predictions * masks\n",
                "    masked_targets = targets * masks\n",
                "    \n",
                "    # Calculate squared differences\n",
                "    squared_diff = (masked_predictions - masked_targets) ** 2\n",
                "    \n",
                "    # Sum over all dimensions and divide by number of non-missing values\n",
                "    total_loss = torch.sum(squared_diff)\n",
                "    total_count = torch.sum(masks)\n",
                "    \n",
                "    if total_count > 0:\n",
                "        return total_loss / total_count\n",
                "    else:\n",
                "        return torch.tensor(0.0, device=predictions.device)\n",
                "\n",
                "\n",
                "def train_one_epoch(model, loader, optimizer, device):\n",
                "    \"\"\"Perform one full training pass over the dataset.\"\"\"\n",
                "    model.train()\n",
                "    total_loss = 0\n",
                "    total_samples = 0\n",
                "    \n",
                "    for data in tqdm(loader, desc=\"Training\", leave=False):\n",
                "        data = data.to(device)\n",
                "        optimizer.zero_grad()\n",
                "        \n",
                "        out = model(data)\n",
                "        loss = masked_mse_loss(out, data.y, data.mask)\n",
                "        \n",
                "        loss.backward()\n",
                "        optimizer.step()\n",
                "        \n",
                "        total_loss += loss.item() * data.num_graphs\n",
                "        total_samples += data.num_graphs\n",
                "        \n",
                "    return total_loss / total_samples\n",
                "\n",
                "\n",
                "@torch.no_grad()\n",
                "def evaluate(model, loader, device):\n",
                "    \"\"\"Evaluate the model on a dataset.\"\"\"\n",
                "    model.eval()\n",
                "    total_loss = 0\n",
                "    total_samples = 0\n",
                "    \n",
                "    all_preds = []\n",
                "    all_targets = []\n",
                "    all_masks = []\n",
                "    \n",
                "    for data in tqdm(loader, desc=\"Evaluating\", leave=False):\n",
                "        data = data.to(device)\n",
                "        out = model(data)\n",
                "        \n",
                "        loss = masked_mse_loss(out, data.y, data.mask)\n",
                "        total_loss += loss.item() * data.num_graphs\n",
                "        total_samples += data.num_graphs\n",
                "        \n",
                "        all_preds.append(out.cpu())\n",
                "        all_targets.append(data.y.cpu())\n",
                "        all_masks.append(data.mask.cpu())\n",
                "\n",
                "    avg_loss = total_loss / total_samples\n",
                "    \n",
                "    # Calculate per-property RMSE\n",
                "    preds = torch.cat(all_preds, dim=0)  # Shape: (N, 5)\n",
                "    targets = torch.cat(all_targets, dim=0)  # Shape: (N, 5)\n",
                "    masks = torch.cat(all_masks, dim=0)  # Shape: (N, 5)\n",
                "    \n",
                "    property_names = CONFIG.TARGET_PROPERTIES\n",
                "    rmses = {}\n",
                "    \n",
                "    for i, prop_name in enumerate(property_names):\n",
                "        prop_mask = masks[:, i]\n",
                "        if prop_mask.sum() > 0:  # Only calculate if we have non-missing values\n",
                "            prop_preds = preds[:, i][prop_mask == 1]\n",
                "            prop_targets = targets[:, i][prop_mask == 1]\n",
                "            rmse = torch.sqrt(torch.mean((prop_preds - prop_targets) ** 2))\n",
                "            rmses[prop_name] = rmse.item()\n",
                "        else:\n",
                "            rmses[prop_name] = float('nan')\n",
                "    \n",
                "    return avg_loss, rmses, (preds.numpy(), targets.numpy(), masks.numpy())\n",
                "\n",
                "\n",
                "@torch.no_grad()\n",
                "def predict(model, loader, device):\n",
                "    \"\"\"Generate predictions for test data.\"\"\"\n",
                "    model.eval()\n",
                "    all_ids = []\n",
                "    all_preds = []\n",
                "    \n",
                "    for data in tqdm(loader, desc=\"Predicting\", leave=False):\n",
                "        data = data.to(device)\n",
                "        out = model(data)\n",
                "        \n",
                "        all_ids.extend(data.id)\n",
                "        all_preds.append(out.cpu())\n",
                "    \n",
                "    predictions = torch.cat(all_preds, dim=0).numpy()\n",
                "    \n",
                "    return all_ids, predictions\n",
                "\n",
                "print(\"Training functions defined!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Competition Metrics\n",
                "\n",
                "Implement the competition's weighted MAE metric."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def weighted_mae(predictions, targets, masks=None):\n",
                "    \"\"\"Calculate the weighted Mean Absolute Error (wMAE) as defined in the competition.\"\"\"\n",
                "    # Convert to numpy if needed\n",
                "    if torch.is_tensor(predictions):\n",
                "        predictions = predictions.detach().cpu().numpy()\n",
                "    if torch.is_tensor(targets):\n",
                "        targets = targets.detach().cpu().numpy()\n",
                "    if masks is not None and torch.is_tensor(masks):\n",
                "        masks = masks.detach().cpu().numpy()\n",
                "    \n",
                "    # If no masks provided, assume all values are present\n",
                "    if masks is None:\n",
                "        masks = np.ones_like(predictions)\n",
                "    \n",
                "    # Calculate weights for each property\n",
                "    weights = []\n",
                "    total_samples = predictions.shape[0]\n",
                "    \n",
                "    for j in range(5):  # 5 properties\n",
                "        # Count non-missing values for this property\n",
                "        N_j = np.sum(masks[:, j])\n",
                "        \n",
                "        if N_j == 0:\n",
                "            weights.append(0.0)\n",
                "            continue\n",
                "        \n",
                "        # Calculate range R_j for this property (using only non-missing values)\n",
                "        valid_mask = masks[:, j] == 1\n",
                "        if np.sum(valid_mask) > 1:\n",
                "            property_values = targets[valid_mask, j]\n",
                "            R_j = np.max(property_values) - np.min(property_values)\n",
                "            if R_j == 0:\n",
                "                R_j = 1.0  # Avoid division by zero\n",
                "        else:\n",
                "            R_j = 1.0\n",
                "        \n",
                "        # Calculate weight: w_j = (5 / sqrt(N_j)) / R_j\n",
                "        w_j = (5.0 / np.sqrt(N_j)) / R_j\n",
                "        weights.append(w_j)\n",
                "    \n",
                "    # Normalize weights so they sum to 5\n",
                "    weights = np.array(weights)\n",
                "    if np.sum(weights) > 0:\n",
                "        weights = weights * (5.0 / np.sum(weights))\n",
                "    \n",
                "    # Calculate weighted MAE\n",
                "    total_weighted_error = 0.0\n",
                "    total_count = 0\n",
                "    \n",
                "    for i in range(total_samples):\n",
                "        for j in range(5):\n",
                "            if masks[i, j] == 1:  # Only consider non-missing values\n",
                "                error = abs(predictions[i, j] - targets[i, j])\n",
                "                weighted_error = weights[j] * error\n",
                "                total_weighted_error += weighted_error\n",
                "                total_count += 1\n",
                "    \n",
                "    if total_count == 0:\n",
                "        return float('inf')\n",
                "    \n",
                "    wmae = total_weighted_error / total_count\n",
                "    return wmae\n",
                "\n",
                "\n",
                "def print_competition_metrics(predictions, targets, masks=None):\n",
                "    \"\"\"Print comprehensive metrics in a nice format.\"\"\"\n",
                "    wmae = weighted_mae(predictions, targets, masks)\n",
                "    \n",
                "    print(\"\\n\" + \"=\"*60)\n",
                "    print(\"COMPETITION METRICS\")\n",
                "    print(\"=\"*60)\n",
                "    print(f\"Weighted MAE (Competition Metric): {wmae:.6f}\")\n",
                "    \n",
                "    # Per-property metrics\n",
                "    if masks is None:\n",
                "        masks = np.ones_like(predictions)\n",
                "    \n",
                "    print(\"\\nPer-Property Metrics:\")\n",
                "    print(\"-\"*60)\n",
                "    print(f\"{'Property':<10} {'Count':<8} {'MAE':<12} {'RMSE':<12} {'Range':<12}\")\n",
                "    print(\"-\"*60)\n",
                "    \n",
                "    for j, prop_name in enumerate(CONFIG.TARGET_PROPERTIES):\n",
                "        valid_mask = masks[:, j] == 1\n",
                "        count = np.sum(valid_mask)\n",
                "        \n",
                "        if count > 0:\n",
                "            pred_vals = predictions[valid_mask, j]\n",
                "            true_vals = targets[valid_mask, j]\n",
                "            \n",
                "            mae = np.mean(np.abs(pred_vals - true_vals))\n",
                "            rmse = np.sqrt(np.mean((pred_vals - true_vals) ** 2))\n",
                "            prop_range = np.max(true_vals) - np.min(true_vals) if len(true_vals) > 1 else 0.0\n",
                "            \n",
                "            print(f\"{prop_name:<10} {count:<8} {mae:<12.6f} {rmse:<12.6f} {prop_range:<12.6f}\")\n",
                "        else:\n",
                "            print(f\"{prop_name:<10} {count:<8} {'N/A':<12} {'N/A':<12} {'N/A':<12}\")\n",
                "    \n",
                "    print(\"=\"*60)\n",
                "\n",
                "print(\"Competition metrics functions defined!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Data Preparation and Training Loop\n",
                "\n",
                "Prepare the datasets and train the model."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Prepare datasets\n",
                "def prepare_datasets(train_df, test_df, val_split=0.2):\n",
                "    \"\"\"Prepare datasets for training, validation, and testing.\"\"\"\n",
                "    # Create datasets\n",
                "    full_train_dataset = PolymerDataset(train_df, is_test=False)\n",
                "    test_dataset = PolymerDataset(test_df, is_test=True)\n",
                "    \n",
                "    # Filter out invalid SMILES from training data\n",
                "    valid_indices = []\n",
                "    print(\"Filtering invalid SMILES...\")\n",
                "    for i in tqdm(range(len(full_train_dataset))):\n",
                "        data = full_train_dataset.get(i)\n",
                "        if data is not None:\n",
                "            valid_indices.append(i)\n",
                "    \n",
                "    if len(valid_indices) != len(full_train_dataset):\n",
                "        print(f\"Warning: Filtered out {len(full_train_dataset) - len(valid_indices)} invalid SMILES from training data.\")\n",
                "    \n",
                "    # Create clean training dataset\n",
                "    train_df_clean = train_df.iloc[valid_indices].reset_index(drop=True)\n",
                "    clean_train_dataset = PolymerDataset(train_df_clean, is_test=False)\n",
                "    \n",
                "    # Split training data into train and validation\n",
                "    train_size = int((1.0 - val_split) * len(clean_train_dataset))\n",
                "    val_size = len(clean_train_dataset) - train_size\n",
                "    train_dataset, val_dataset = random_split(clean_train_dataset, [train_size, val_size])\n",
                "    \n",
                "    print(f\"\\nDataset sizes:\")\n",
                "    print(f\"  Training: {len(train_dataset)}\")\n",
                "    print(f\"  Validation: {len(val_dataset)}\")\n",
                "    print(f\"  Test: {len(test_dataset)}\")\n",
                "    \n",
                "    return train_dataset, val_dataset, test_dataset\n",
                "\n",
                "# Prepare datasets\n",
                "train_dataset, val_dataset, test_dataset = prepare_datasets(\n",
                "    train_df, test_df, val_split=CONFIG.VAL_SPLIT_FRACTION\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create data loaders\n",
                "train_loader = DataLoader(train_dataset, batch_size=CONFIG.BATCH_SIZE, shuffle=True)\n",
                "val_loader = DataLoader(val_dataset, batch_size=CONFIG.BATCH_SIZE, shuffle=False)\n",
                "test_loader = DataLoader(test_dataset, batch_size=CONFIG.BATCH_SIZE, shuffle=False)\n",
                "\n",
                "print(f\"Data loaders created:\")\n",
                "print(f\"  Train batches: {len(train_loader)}\")\n",
                "print(f\"  Val batches: {len(val_loader)}\")\n",
                "print(f\"  Test batches: {len(test_loader)}\")\n",
                "\n",
                "# Get feature dimensions\n",
                "first_data = train_dataset[0]\n",
                "num_atom_features = first_data.num_atom_features\n",
                "print(f\"  Atom features: {num_atom_features}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialize model, optimizer, and scheduler\n",
                "model = PolymerGCN(\n",
                "    num_atom_features=num_atom_features,\n",
                "    hidden_channels=CONFIG.HIDDEN_CHANNELS,\n",
                "    num_gcn_layers=CONFIG.NUM_GCN_LAYERS\n",
                ").to(CONFIG.DEVICE)\n",
                "\n",
                "optimizer = torch.optim.Adam(\n",
                "    model.parameters(),\n",
                "    lr=CONFIG.LEARNING_RATE,\n",
                "    weight_decay=CONFIG.WEIGHT_DECAY\n",
                ")\n",
                "\n",
                "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
                "    optimizer, mode='min', factor=0.5, patience=5\n",
                ")\n",
                "\n",
                "print(f\"Model initialized:\")\n",
                "print(f\"  Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
                "print(f\"  Device: {CONFIG.DEVICE}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Training loop\n",
                "print(f\"Starting training for {CONFIG.NUM_EPOCHS} epochs...\")\n",
                "print(\"=\"*80)\n",
                "\n",
                "# Training history\n",
                "history = {\n",
                "    'train_losses': [],\n",
                "    'val_losses': [],\n",
                "    'val_rmses': [],\n",
                "    'learning_rates': [],\n",
                "    'wmae_scores': []\n",
                "}\n",
                "\n",
                "best_val_loss = float('inf')\n",
                "best_wmae = float('inf')\n",
                "patience_counter = 0\n",
                "early_stopping_patience = 15\n",
                "\n",
                "for epoch in range(1, CONFIG.NUM_EPOCHS + 1):\n",
                "    # Train\n",
                "    train_loss = train_one_epoch(model, train_loader, optimizer, CONFIG.DEVICE)\n",
                "    history['train_losses'].append(train_loss)\n",
                "    \n",
                "    # Validate\n",
                "    val_loss, val_rmses, (val_preds, val_targets, val_masks) = evaluate(model, val_loader, CONFIG.DEVICE)\n",
                "    history['val_losses'].append(val_loss)\n",
                "    history['val_rmses'].append(val_rmses)\n",
                "    history['learning_rates'].append(optimizer.param_groups[0]['lr'])\n",
                "    \n",
                "    # Calculate competition metric\n",
                "    wmae = weighted_mae(val_preds, val_targets, val_masks)\n",
                "    history['wmae_scores'].append(wmae)\n",
                "    \n",
                "    # Update learning rate\n",
                "    scheduler.step(val_loss)\n",
                "    \n",
                "    # Print progress\n",
                "    rmse_str = \", \".join([f\"{k}: {v:.4f}\" for k, v in val_rmses.items() if not np.isnan(v)])\n",
                "    print(f\"Epoch {epoch:3d} | Train: {train_loss:.4f} | Val: {val_loss:.4f} | wMAE: {wmae:.6f} | LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
                "    print(f\"         RMSE: {rmse_str}\")\n",
                "    \n",
                "    # Save best model\n",
                "    if wmae < best_wmae:\n",
                "        best_wmae = wmae\n",
                "        best_val_loss = val_loss\n",
                "        patience_counter = 0\n",
                "        \n",
                "        # Save model state\n",
                "        torch.save({\n",
                "            'epoch': epoch,\n",
                "            'model_state_dict': model.state_dict(),\n",
                "            'optimizer_state_dict': optimizer.state_dict(),\n",
                "            'val_loss': val_loss,\n",
                "            'wmae': wmae,\n",
                "            'val_rmses': val_rmses,\n",
                "        }, 'best_model.pt')\n",
                "        \n",
                "        print(f\"         -> New best wMAE! Model saved.\")\n",
                "    else:\n",
                "        patience_counter += 1\n",
                "    \n",
                "    # Early stopping\n",
                "    if patience_counter >= early_stopping_patience:\n",
                "        print(f\"\\nEarly stopping triggered after {patience_counter} epochs without improvement.\")\n",
                "        break\n",
                "    \n",
                "    print(\"-\" * 80)\n",
                "\n",
                "print(f\"\\nTraining complete!\")\n",
                "print(f\"Best validation wMAE: {best_wmae:.6f}\")\n",
                "print(f\"Best validation loss: {best_val_loss:.4f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Plot training history\n",
                "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
                "\n",
                "# Training and validation loss\n",
                "axes[0, 0].plot(history['train_losses'], label='Train Loss', alpha=0.8)\n",
                "axes[0, 0].plot(history['val_losses'], label='Val Loss', alpha=0.8)\n",
                "axes[0, 0].set_title('Training and Validation Loss')\n",
                "axes[0, 0].set_xlabel('Epoch')\n",
                "axes[0, 0].set_ylabel('Loss')\n",
                "axes[0, 0].legend()\n",
                "axes[0, 0].grid(True, alpha=0.3)\n",
                "\n",
                "# Competition metric (wMAE)\n",
                "axes[0, 1].plot(history['wmae_scores'], label='wMAE', color='red', alpha=0.8)\n",
                "axes[0, 1].set_title('Competition Metric (wMAE)')\n",
                "axes[0, 1].set_xlabel('Epoch')\n",
                "axes[0, 1].set_ylabel('wMAE')\n",
                "axes[0, 1].legend()\n",
                "axes[0, 1].grid(True, alpha=0.3)\n",
                "\n",
                "# Learning rate\n",
                "axes[1, 0].plot(history['learning_rates'], label='Learning Rate', color='green', alpha=0.8)\n",
                "axes[1, 0].set_title('Learning Rate Schedule')\n",
                "axes[1, 0].set_xlabel('Epoch')\n",
                "axes[1, 0].set_ylabel('Learning Rate')\n",
                "axes[1, 0].set_yscale('log')\n",
                "axes[1, 0].legend()\n",
                "axes[1, 0].grid(True, alpha=0.3)\n",
                "\n",
                "# RMSE by property\n",
                "for prop in CONFIG.TARGET_PROPERTIES:\n",
                "    rmse_values = [rmse_dict.get(prop, np.nan) for rmse_dict in history['val_rmses']]\n",
                "    # Only plot if we have valid values\n",
                "    if not all(np.isnan(rmse_values)):\n",
                "        axes[1, 1].plot(rmse_values, label=prop, alpha=0.8)\n",
                "\n",
                "axes[1, 1].set_title('Validation RMSE by Property')\n",
                "axes[1, 1].set_xlabel('Epoch')\n",
                "axes[1, 1].set_ylabel('RMSE')\n",
                "axes[1, 1].legend()\n",
                "axes[1, 1].grid(True, alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. Evaluation and Submission\n",
                "\n",
                "Load the best model and generate predictions for the test set."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load the best model\n",
                "print(\"Loading best model...\")\n",
                "checkpoint = torch.load('best_model.pt', map_location=CONFIG.DEVICE, weights_only=False)\n",
                "model.load_state_dict(checkpoint['model_state_dict'])\n",
                "\n",
                "print(f\"Best model loaded:\")\n",
                "print(f\"  Epoch: {checkpoint['epoch']}\")\n",
                "print(f\"  Validation Loss: {checkpoint['val_loss']:.4f}\")\n",
                "print(f\"  Competition wMAE: {checkpoint['wmae']:.6f}\")\n",
                "\n",
                "# Final evaluation on validation set\n",
                "print(\"\\nFinal validation evaluation:\")\n",
                "val_loss, val_rmses, (val_preds, val_targets, val_masks) = evaluate(model, val_loader, CONFIG.DEVICE)\n",
                "print_competition_metrics(val_preds, val_targets, val_masks)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Generate predictions for test set\n",
                "print(\"Generating predictions for test set...\")\n",
                "test_ids, test_predictions = predict(model, test_loader, CONFIG.DEVICE)\n",
                "\n",
                "# Create submission DataFrame\n",
                "submission_df = pd.DataFrame({\n",
                "    'id': [int(id_val) if hasattr(id_val, 'item') else int(id_val) for id_val in test_ids],\n",
                "    'Tg': test_predictions[:, 0],\n",
                "    'FFV': test_predictions[:, 1],\n",
                "    'Tc': test_predictions[:, 2],\n",
                "    'Density': test_predictions[:, 3],\n",
                "    'Rg': test_predictions[:, 4]\n",
                "})\n",
                "\n",
                "# Save submission\n",
                "submission_filename = f\"submission_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
                "submission_df.to_csv(submission_filename, index=False)\n",
                "\n",
                "print(f\"\\nSubmission saved to: {submission_filename}\")\n",
                "print(f\"Submission shape: {submission_df.shape}\")\n",
                "print(\"\\nSample predictions:\")\n",
                "display(submission_df.head())\n",
                "\n",
                "# Verify submission format\n",
                "expected_columns = ['id', 'Tg', 'FFV', 'Tc', 'Density', 'Rg']\n",
                "if list(submission_df.columns) == expected_columns:\n",
                "    print(\" Submission format is correct!\")\n",
                "else:\n",
                "    print(\" Submission format is incorrect!\")\n",
                "    print(f\"Expected: {expected_columns}\")\n",
                "    print(f\"Got: {list(submission_df.columns)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 10. Results and Analysis\n",
                "\n",
                "Analyze the model performance and predictions."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Analyze prediction distributions\n",
                "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
                "axes = axes.flatten()\n",
                "\n",
                "for i, prop in enumerate(CONFIG.TARGET_PROPERTIES):\n",
                "    # Get validation predictions and targets for this property\n",
                "    prop_mask = val_masks[:, i] == 1\n",
                "    \n",
                "    if np.sum(prop_mask) > 0:\n",
                "        pred_vals = val_preds[prop_mask, i]\n",
                "        true_vals = val_targets[prop_mask, i]\n",
                "        \n",
                "        # Scatter plot: predictions vs targets\n",
                "        axes[i].scatter(true_vals, pred_vals, alpha=0.6, s=20)\n",
                "        \n",
                "        # Perfect prediction line\n",
                "        min_val = min(true_vals.min(), pred_vals.min())\n",
                "        max_val = max(true_vals.max(), pred_vals.max())\n",
                "        axes[i].plot([min_val, max_val], [min_val, max_val], 'r--', alpha=0.8, label='Perfect Prediction')\n",
                "        \n",
                "        # Calculate R\n",
                "        r2 = 1 - np.sum((true_vals - pred_vals)**2) / np.sum((true_vals - np.mean(true_vals))**2)\n",
                "        \n",
                "        axes[i].set_title(f'{prop} (n={len(pred_vals)}, R={r2:.3f})')\n",
                "        axes[i].set_xlabel('True Values')\n",
                "        axes[i].set_ylabel('Predicted Values')\n",
                "        axes[i].legend()\n",
                "        axes[i].grid(True, alpha=0.3)\n",
                "    else:\n",
                "        axes[i].text(0.5, 0.5, f'No data for {prop}', \n",
                "                    horizontalalignment='center', verticalalignment='center',\n",
                "                    transform=axes[i].transAxes, fontsize=14)\n",
                "        axes[i].set_title(f'{prop} (No Data)')\n",
                "\n",
                "# Remove empty subplot\n",
                "fig.delaxes(axes[5])\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Analyze test predictions\n",
                "print(\"Test Prediction Statistics:\")\n",
                "print(\"=\" * 50)\n",
                "\n",
                "test_stats = submission_df[CONFIG.TARGET_PROPERTIES].describe()\n",
                "display(test_stats)\n",
                "\n",
                "# Plot test prediction distributions\n",
                "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
                "axes = axes.flatten()\n",
                "\n",
                "for i, prop in enumerate(CONFIG.TARGET_PROPERTIES):\n",
                "    values = submission_df[prop]\n",
                "    \n",
                "    axes[i].hist(values, bins=20, alpha=0.7, edgecolor='black')\n",
                "    axes[i].set_title(f'{prop} Test Predictions\\n(mean={values.mean():.3f}, std={values.std():.3f})')\n",
                "    axes[i].set_xlabel(prop)\n",
                "    axes[i].set_ylabel('Frequency')\n",
                "    axes[i].grid(True, alpha=0.3)\n",
                "\n",
                "# Remove empty subplot\n",
                "fig.delaxes(axes[5])\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Summary\n",
                "print(\"\\n\" + \"=\"*80)\n",
                "print(\"NEURIPS OPEN POLYMER PREDICTION 2025 - SOLUTION SUMMARY\")\n",
                "print(\"=\"*80)\n",
                "print(f\"Model Architecture: Graph Convolutional Network\")\n",
                "print(f\"  - Input features: {num_atom_features} atom features\")\n",
                "print(f\"  - Hidden channels: {CONFIG.HIDDEN_CHANNELS}\")\n",
                "print(f\"  - GCN layers: {CONFIG.NUM_GCN_LAYERS}\")\n",
                "print(f\"  - Output: 5 polymer properties\")\n",
                "print(f\"  - Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
                "\n",
                "print(f\"\\nTraining Configuration:\")\n",
                "print(f\"  - Training samples: {len(train_dataset):,}\")\n",
                "print(f\"  - Validation samples: {len(val_dataset):,}\")\n",
                "print(f\"  - Test samples: {len(test_dataset):,}\")\n",
                "print(f\"  - Batch size: {CONFIG.BATCH_SIZE}\")\n",
                "print(f\"  - Learning rate: {CONFIG.LEARNING_RATE}\")\n",
                "print(f\"  - Epochs trained: {len(history['train_losses'])}\")\n",
                "\n",
                "print(f\"\\nBest Performance:\")\n",
                "print(f\"  - Validation Loss: {checkpoint['val_loss']:.4f}\")\n",
                "print(f\"  - Competition wMAE: {checkpoint['wmae']:.6f}\")\n",
                "\n",
                "print(f\"\\nSubmission:\")\n",
                "print(f\"  - File: {submission_filename}\")\n",
                "print(f\"  - Format:  Correct\")\n",
                "print(f\"  - Ready for upload:  Yes\")\n",
                "\n",
                "print(\"\\nKey Features:\")\n",
                "print(\"   Multi-target prediction (5 properties simultaneously)\")\n",
                "print(\"   Missing value handling with binary masks\")\n",
                "print(\"   Competition metric implementation (weighted MAE)\")\n",
                "print(\"   Advanced molecular featurization (26 atom features)\")\n",
                "print(\"   Early stopping and learning rate scheduling\")\n",
                "print(\"   Comprehensive evaluation and visualization\")\n",
                "\n",
                "print(\"=\"*80)\n",
                "print(\" SOLUTION COMPLETE - READY FOR SUBMISSION! \")\n",
                "print(\"=\"*80)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}