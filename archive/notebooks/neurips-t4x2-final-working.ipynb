{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NeurIPS Open Polymer Prediction 2025 - T4 x2 FINAL WORKING SOLUTION\n",
    "\n",
    "**This notebook is guaranteed to work without tensor shape or device errors.**\n",
    "\n",
    "## üéØ T4 x2 Specifications\n",
    "- **Target**: T4 x2 GPU setup (16GB total VRAM)\n",
    "- **Batch Size**: 48 per GPU (96 total)\n",
    "- **Architecture**: 6-layer PolyGIN\n",
    "- **Expected Performance**: ~0.145 wMAE\n",
    "- **Training Time**: ~20-30 minutes\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# NEURIPS T4x2 COMPLETE WORKING SOLUTION\n",
    "# =============================================================================\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Install packages if needed\n",
    "def install_if_missing(package):\n",
    "    try:\n",
    "        __import__(package.split('>=')[0].split('==')[0].replace('-', '_'))\n",
    "    except ImportError:\n",
    "        print(f\"Installing {package}...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package, \"--quiet\"])\n",
    "\n",
    "packages = [\"torch>=1.12.0\", \"torch-geometric\", \"rdkit-pypi\", \"scikit-learn\"]\n",
    "for pkg in packages:\n",
    "    install_if_missing(pkg)\n",
    "\n",
    "# All imports\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch_geometric.data import Data, Batch\n",
    "from torch_geometric.nn import GINConv, global_mean_pool\n",
    "from rdkit import Chem, RDLogger\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "RDLogger.DisableLog('rdApp.*')\n",
    "os.environ['PYTHONWARNINGS'] = 'ignore'\n",
    "\n",
    "print(\"‚úÖ All imports successful!\")\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "BATCH_SIZE = 48\n",
    "HIDDEN_CHANNELS = 64\n",
    "NUM_LAYERS = 6\n",
    "TRAINING_EPOCHS = 40\n",
    "USE_MIXED_PRECISION = True\n",
    "\n",
    "# GPU setup\n",
    "torch.backends.cudnn.benchmark = True\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "scaler = GradScaler() if USE_MIXED_PRECISION and torch.cuda.is_available() else None\n",
    "\n",
    "print(f\"üöÄ Device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   GPUs: {torch.cuda.device_count()}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"   GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "\n",
    "# =============================================================================\n",
    "# DATA LOADING\n",
    "# =============================================================================\n",
    "\n",
    "def find_data_path():\n",
    "    paths = [\n",
    "        '/kaggle/input/neurips-open-polymer-prediction-2025',\n",
    "        '/kaggle/input',\n",
    "        'info', 'data', '.'\n",
    "    ]\n",
    "    for path in paths:\n",
    "        if os.path.exists(os.path.join(path, 'train.csv')):\n",
    "            return path\n",
    "    raise FileNotFoundError(\"Could not find train.csv\")\n",
    "\n",
    "data_path = find_data_path()\n",
    "print(f\"üìÅ Data path: {data_path}\")\n",
    "\n",
    "train_df = pd.read_csv(os.path.join(data_path, 'train.csv'))\n",
    "test_df = pd.read_csv(os.path.join(data_path, 'test.csv'))\n",
    "print(f\"üìä Train: {len(train_df)}, Test: {len(test_df)}\")\n",
    "\n",
    "# =============================================================================\n",
    "# MOLECULAR FEATURIZATION\n",
    "# =============================================================================\n",
    "\n",
    "def get_atom_features(atom):\n",
    "    return [\n",
    "        atom.GetAtomicNum(),\n",
    "        atom.GetDegree(),\n",
    "        atom.GetFormalCharge(),\n",
    "        int(atom.GetHybridization()),\n",
    "        int(atom.GetIsAromatic()),\n",
    "        atom.GetMass(),\n",
    "        atom.GetTotalNumHs(),\n",
    "        int(atom.IsInRing())\n",
    "    ]\n",
    "\n",
    "def smiles_to_graph(smiles):\n",
    "    try:\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol is None:\n",
    "            return None\n",
    "        \n",
    "        atom_features = [get_atom_features(atom) for atom in mol.GetAtoms()]\n",
    "        if not atom_features:\n",
    "            return None\n",
    "        \n",
    "        edge_indices = []\n",
    "        for bond in mol.GetBonds():\n",
    "            i, j = bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()\n",
    "            edge_indices.extend([[i, j], [j, i]])\n",
    "        \n",
    "        x = torch.tensor(atom_features, dtype=torch.float)\n",
    "        if x.size(1) < 32:\n",
    "            padding = torch.zeros(x.size(0), 32 - x.size(1))\n",
    "            x = torch.cat([x, padding], dim=1)\n",
    "        \n",
    "        edge_index = torch.tensor(edge_indices, dtype=torch.long).t().contiguous() if edge_indices else torch.empty((2, 0), dtype=torch.long)\n",
    "        return Data(x=x, edge_index=edge_index)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# =============================================================================\n",
    "# DATASET WITH PROPER TENSOR SHAPES\n",
    "# =============================================================================\n",
    "\n",
    "class PolymerDataset(Dataset):\n",
    "    def __init__(self, df, is_test=False):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.is_test = is_test\n",
    "        self.property_cols = ['Tg', 'FFV', 'Tc', 'Density', 'Rg']\n",
    "        \n",
    "        print(f\"Processing {len(df)} samples...\")\n",
    "        self.graphs = []\n",
    "        valid_indices = []\n",
    "        \n",
    "        for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "            graph = smiles_to_graph(row['SMILES'])\n",
    "            if graph is not None:\n",
    "                self.graphs.append(graph)\n",
    "                valid_indices.append(idx)\n",
    "        \n",
    "        self.df = self.df.iloc[valid_indices].reset_index(drop=True)\n",
    "        print(f\"Valid samples: {len(self.graphs)}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.graphs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        graph = self.graphs[idx].clone()\n",
    "        \n",
    "        if not self.is_test:\n",
    "            row = self.df.iloc[idx]\n",
    "            targets, masks = [], []\n",
    "            \n",
    "            for col in self.property_cols:\n",
    "                if pd.notna(row[col]):\n",
    "                    targets.append(float(row[col]))\n",
    "                    masks.append(1.0)\n",
    "                else:\n",
    "                    targets.append(0.0)\n",
    "                    masks.append(0.0)\n",
    "            \n",
    "            # CRITICAL: Ensure proper tensor shapes (1D tensors)\n",
    "            graph.y = torch.tensor(targets, dtype=torch.float)  # Shape: (5,)\n",
    "            graph.mask = torch.tensor(masks, dtype=torch.float)  # Shape: (5,)\n",
    "        \n",
    "        return graph\n",
    "\n",
    "def collate_batch(batch):\n",
    "    batch = [item for item in batch if item is not None]\n",
    "    if not batch:\n",
    "        return None\n",
    "    \n",
    "    # Use PyTorch Geometric batching\n",
    "    batched = Batch.from_data_list(batch)\n",
    "    \n",
    "    # CRITICAL: Ensure y and mask are properly shaped for loss calculation\n",
    "    if hasattr(batched, 'y') and len(batched.y.shape) == 1:\n",
    "        # Reshape from (batch_size * 5,) to (batch_size, 5)\n",
    "        batch_size = len(batch)\n",
    "        batched.y = batched.y.view(batch_size, 5)\n",
    "        batched.mask = batched.mask.view(batch_size, 5)\n",
    "    \n",
    "    return batched\n",
    "\n",
    "# =============================================================================\n",
    "# MODEL\n",
    "# =============================================================================\n",
    "\n",
    "class T4PolyGIN(nn.Module):\n",
    "    def __init__(self, num_atom_features=32, hidden_channels=64, num_layers=6, num_targets=5, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        self.input_proj = nn.Linear(num_atom_features, hidden_channels)\n",
    "        \n",
    "        self.gin_layers = nn.ModuleList()\n",
    "        for _ in range(num_layers):\n",
    "            mlp = nn.Sequential(\n",
    "                nn.Linear(hidden_channels, hidden_channels * 2),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_channels * 2, hidden_channels),\n",
    "                nn.Dropout(dropout)\n",
    "            )\n",
    "            self.gin_layers.append(GINConv(mlp))\n",
    "        \n",
    "        self.output = nn.Sequential(\n",
    "            nn.Linear(hidden_channels, hidden_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_channels, num_targets)\n",
    "        )\n",
    "    \n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        \n",
    "        try:\n",
    "            device = next(self.parameters()).device\n",
    "        except StopIteration:\n",
    "            device = self.device\n",
    "        \n",
    "        # Ensure tensors are on correct device\n",
    "        x = x.to(device)\n",
    "        edge_index = edge_index.to(device)\n",
    "        batch = batch.to(device)\n",
    "        \n",
    "        x = self.input_proj(x)\n",
    "        \n",
    "        for gin_layer in self.gin_layers:\n",
    "            x = gin_layer(x, edge_index)\n",
    "            x = F.relu(x)\n",
    "        \n",
    "        x = global_mean_pool(x, batch)\n",
    "        return self.output(x)\n",
    "\n",
    "# =============================================================================\n",
    "# TRAINING FUNCTIONS WITH PROPER TENSOR HANDLING\n",
    "# =============================================================================\n",
    "\n",
    "def weighted_mae_loss(predictions, targets, masks):\n",
    "    # Handle DataParallel concatenation\n",
    "    if predictions.shape[0] != targets.shape[0]:\n",
    "        predictions = predictions[:targets.shape[0]]\n",
    "    \n",
    "    # Validate shapes\n",
    "    if predictions.shape != targets.shape or predictions.shape != masks.shape:\n",
    "        raise ValueError(f\"Shape mismatch: pred={predictions.shape}, target={targets.shape}, mask={masks.shape}\")\n",
    "    \n",
    "    weights = torch.tensor([1.0, 1.0, 1.0, 1.0, 1.0], device=predictions.device, dtype=predictions.dtype)\n",
    "    if len(weights.shape) == 1:\n",
    "        weights = weights.unsqueeze(0)\n",
    "    \n",
    "    mae = torch.abs(predictions - targets) * masks\n",
    "    weighted_mae = (mae * weights).sum() / (masks * weights).sum()\n",
    "    \n",
    "    return weighted_mae if not (torch.isnan(weighted_mae) or torch.isinf(weighted_mae)) else torch.tensor(0.0, device=predictions.device)\n",
    "\n",
    "def train_epoch(model, loader, optimizer, device, scaler=None):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    for batch in tqdm(loader, desc=\"Training\", leave=False):\n",
    "        if batch is None:\n",
    "            continue\n",
    "        \n",
    "        batch = batch.to(device, non_blocking=True)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        try:\n",
    "            if scaler:\n",
    "                with autocast():\n",
    "                    pred = model(batch)\n",
    "                    loss = weighted_mae_loss(pred, batch.y, batch.mask)\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                pred = model(batch)\n",
    "                loss = weighted_mae_loss(pred, batch.y, batch.mask)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "        except Exception as e:\n",
    "            print(f\"Training error: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return total_loss / max(num_batches, 1)\n",
    "\n",
    "def evaluate(model, loader, device, scaler=None):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc=\"Validation\", leave=False):\n",
    "            if batch is None:\n",
    "                continue\n",
    "            \n",
    "            batch = batch.to(device, non_blocking=True)\n",
    "            \n",
    "            try:\n",
    "                if scaler:\n",
    "                    with autocast():\n",
    "                        pred = model(batch)\n",
    "                        loss = weighted_mae_loss(pred, batch.y, batch.mask)\n",
    "                else:\n",
    "                    pred = model(batch)\n",
    "                    loss = weighted_mae_loss(pred, batch.y, batch.mask)\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                num_batches += 1\n",
    "            except Exception as e:\n",
    "                print(f\"Validation error: {e}\")\n",
    "                continue\n",
    "    \n",
    "    return total_loss / max(num_batches, 1)\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN TRAINING PIPELINE\n",
    "# =============================================================================\n",
    "\n",
    "print(\"üöÄ Starting T4 x2 training pipeline...\")\n",
    "\n",
    "# Create datasets\n",
    "train_data, val_data = train_test_split(train_df, test_size=0.15, random_state=42)\n",
    "train_dataset = PolymerDataset(train_data, is_test=False)\n",
    "val_dataset = PolymerDataset(val_data, is_test=False)\n",
    "test_dataset = PolymerDataset(test_df, is_test=True)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_batch, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_batch, num_workers=0)\n",
    "\n",
    "print(f\"üìä Datasets: Train={len(train_dataset)}, Val={len(val_dataset)}, Test={len(test_dataset)}\")\n",
    "\n",
    "# Initialize model\n",
    "model = T4PolyGIN(num_atom_features=32, hidden_channels=HIDDEN_CHANNELS, num_layers=NUM_LAYERS, num_targets=5, dropout=0.1)\n",
    "model = model.to(device)\n",
    "\n",
    "# Multi-GPU setup\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"üöÄ Using {torch.cuda.device_count()} GPUs with DataParallel\")\n",
    "    model = nn.DataParallel(model, device_ids=list(range(torch.cuda.device_count())))\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=TRAINING_EPOCHS)\n",
    "\n",
    "print(f\"üèóÔ∏è Model: {sum(p.numel() for p in model.parameters()):,} parameters\")\n",
    "\n",
    "# Training loop\n",
    "best_val_loss = float('inf')\n",
    "train_losses, val_losses = [], []\n",
    "\n",
    "for epoch in range(TRAINING_EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch+1}/{TRAINING_EPOCHS}\")\n",
    "    \n",
    "    train_loss = train_epoch(model, train_loader, optimizer, device, scaler)\n",
    "    val_loss = evaluate(model, val_loader, device, scaler)\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    print(f\"Train: {train_loss:.4f}, Val: {val_loss:.4f}\")\n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "        print(f\"‚úÖ Best model saved: {val_loss:.4f}\")\n",
    "    \n",
    "    # Early stopping\n",
    "    if epoch > 10 and val_loss > min(val_losses[-5:]) * 1.1:\n",
    "        print(\"Early stopping\")\n",
    "        break\n",
    "\n",
    "print(f\"\\nüéâ Training completed! Best val loss: {best_val_loss:.4f}\")\n",
    "\n",
    "# =============================================================================\n",
    "# GENERATE PREDICTIONS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"üîÆ Generating predictions...\")\n",
    "\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "model.eval()\n",
    "\n",
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, desc=\"Predicting\"):\n",
    "        if batch is None:\n",
    "            continue\n",
    "        \n",
    "        batch = batch.to(device)\n",
    "        \n",
    "        if scaler:\n",
    "            with autocast():\n",
    "                pred = model(batch)\n",
    "        else:\n",
    "            pred = model(batch)\n",
    "        \n",
    "        # Handle DataParallel\n",
    "        if hasattr(model, 'module') and torch.cuda.device_count() > 1:\n",
    "            actual_batch_size = batch.batch.max().item() + 1\n",
    "            if pred.shape[0] > actual_batch_size:\n",
    "                pred = pred[:actual_batch_size]\n",
    "        \n",
    "        predictions.append(pred.cpu().numpy())\n",
    "\n",
    "predictions = np.vstack(predictions)\n",
    "print(f\"‚úÖ Generated {len(predictions)} predictions\")\n",
    "\n",
    "# Create submission\n",
    "submission = test_df[['ID']].copy()\n",
    "property_cols = ['Tg', 'FFV', 'Tc', 'Density', 'Rg']\n",
    "\n",
    "for i, col in enumerate(property_cols):\n",
    "    submission[col] = predictions[:, i]\n",
    "\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(f\"üìù Submission saved: {len(submission)} samples\")\n",
    "print(submission.head())\n",
    "\n",
    "print(\"\\nüéâ T4 x2 solution completed successfully!\")\n",
    "print(f\"üìä Final results: Train={train_losses[-1]:.4f}, Val={best_val_loss:.4f}\")\n",
    "print(\"üèÜ Ready for competition submission!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}