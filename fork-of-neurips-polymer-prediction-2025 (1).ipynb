{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":74608,"databundleVersionId":12966160,"sourceType":"competition"}],"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# NeurIPS Open Polymer Prediction 2025 - Complete Solution\n\nThis notebook contains the complete solution for the NeurIPS Open Polymer Prediction 2025 challenge.\n\n## Challenge Overview\n\n- **Task**: Predict 5 polymer properties from SMILES strings\n- **Properties**: Tg (glass transition temperature), FFV (fractional free volume), Tc (thermal conductivity), Density, Rg (radius of gyration)\n- **Evaluation**: Weighted Mean Absolute Error (wMAE)\n- **Data**: 7,973 training samples with significant missing values\n\n## Table of Contents\n1. [Setup and Imports](#setup)\n2. [Data Loading and Exploration](#data)\n3. [Molecular Featurization](#featurization)\n4. [Dataset Implementation](#dataset)\n5. [Model Architecture](#model)\n6. [Training Implementation](#training)\n7. [Competition Metrics](#metrics)\n8. [Training Loop](#training-loop)\n9. [Evaluation and Submission](#submission)\n10. [Results and Analysis](#results)","metadata":{}},{"cell_type":"markdown","source":"## 1. Setup and Imports\n\nFirst, let's install the required packages and import all necessary libraries.","metadata":{}},{"cell_type":"code","source":"import os\nimport sys\nimport subprocess\n\n# Define required packages\nREQUIRED_PACKAGES = [\n    \"torch\",\n    \"torch-geometric\",\n    \"rdkit-pypi\",\n    \"pandas\",\n    \"numpy\",\n    \"scikit-learn\",\n    \"tqdm\",\n    \"matplotlib\",\n    \"seaborn\"\n]\n\n# Function to check and install missing packages\ndef install_and_restart_if_needed(packages):\n    import pkg_resources\n    missing_packages = []\n    for package in packages:\n        try:\n            # Special handling for package name differences (like rdkit)\n            pkg_resources.get_distribution(package if package != \"rdkit-pypi\" else \"rdkit\")\n        except pkg_resources.DistributionNotFound:\n            missing_packages.append(package)\n    if missing_packages:\n        print(f\"Installing missing packages: {missing_packages}\")\n        # Install with --quiet for a cleaner output, you can drop --quiet if you want detailed logs\n        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--quiet\"] + missing_packages)\n        # After installing, force a kernel restart\n        os._exit(0)\n\n# Run the installer/restarting code\ninstall_and_restart_if_needed(REQUIRED_PACKAGES)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T13:57:45.699684Z","iopub.execute_input":"2025-08-03T13:57:45.700794Z","execution_failed":"2025-08-03T13:57:52.069Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Core imports\nimport os\nimport sys\nimport random\nimport warnings\nfrom datetime import datetime\nfrom typing import List, Dict, Tuple, Optional\n\n# Data handling\nimport pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\n\n# Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# PyTorch and PyTorch Geometric\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import random_split\nfrom torch_geometric.data import Data, Dataset, DataLoader\nfrom torch_geometric.nn import GCNConv, global_mean_pool\n\n# RDKit for chemistry\nfrom rdkit import Chem\nfrom rdkit.Chem import rdchem\nfrom rdkit import RDLogger\n\n# Suppress RDKit warnings\nRDLogger.DisableLog('rdApp.*')\nwarnings.filterwarnings('ignore')\n\n# Set style for plots\nplt.style.use('default')\nsns.set_palette(\"husl\")\n\nprint(\"All imports successful!\")\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"Device available: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T13:58:25.321323Z","iopub.execute_input":"2025-08-03T13:58:25.321620Z","iopub.status.idle":"2025-08-03T13:58:37.740500Z","shell.execute_reply.started":"2025-08-03T13:58:25.321595Z","shell.execute_reply":"2025-08-03T13:58:37.739251Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Configuration\nclass Config:\n    \"\"\"Configuration class for model hyperparameters and settings.\"\"\"\n    \n    def __init__(self):\n        # Device\n        self.DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        \n        # Model hyperparameters\n        self.NUM_EPOCHS = 100\n        self.BATCH_SIZE = 32\n        self.LEARNING_RATE = 5e-4\n        self.WEIGHT_DECAY = 1e-4\n        self.HIDDEN_CHANNELS = 100\n        self.NUM_GCN_LAYERS = 6\n        \n        # Data\n        self.VAL_SPLIT_FRACTION = 0.2\n        self.SEED = 42\n        \n        # Target properties\n        self.TARGET_PROPERTIES = ['Tg', 'FFV', 'Tc', 'Density', 'Rg']\n\n# Create configuration instance\nCONFIG = Config()\n\nprint(f\"Configuration:\")\nprint(f\"  Device: {CONFIG.DEVICE}\")\nprint(f\"  Batch size: {CONFIG.BATCH_SIZE}\")\nprint(f\"  Learning rate: {CONFIG.LEARNING_RATE}\")\nprint(f\"  Hidden channels: {CONFIG.HIDDEN_CHANNELS}\")\nprint(f\"  Target properties: {CONFIG.TARGET_PROPERTIES}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T13:58:37.742689Z","iopub.execute_input":"2025-08-03T13:58:37.743247Z","iopub.status.idle":"2025-08-03T13:58:37.751692Z","shell.execute_reply.started":"2025-08-03T13:58:37.743220Z","shell.execute_reply":"2025-08-03T13:58:37.750445Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Set random seeds for reproducibility\ndef set_seed(seed: int = 42):\n    \"\"\"Set random seeds for reproducibility.\"\"\"\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n\nset_seed(CONFIG.SEED)\nprint(f\"Random seed set to {CONFIG.SEED}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T13:58:37.753253Z","iopub.execute_input":"2025-08-03T13:58:37.753617Z","iopub.status.idle":"2025-08-03T13:58:37.788427Z","shell.execute_reply.started":"2025-08-03T13:58:37.753586Z","shell.execute_reply":"2025-08-03T13:58:37.787178Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2. Data Loading and Exploration\n\nLet's load the competition data and explore its structure.","metadata":{}},{"cell_type":"code","source":"# Load competition data\ndef load_competition_data():\n    \"\"\"Load the competition data files.\"\"\"\n    train_path = \"/kaggle/input/neurips-open-polymer-prediction-2025/train.csv\"\n    test_path = \"/kaggle/input/neurips-open-polymer-prediction-2025/test.csv\"\n    \n    if not os.path.exists(train_path):\n        raise FileNotFoundError(f\"Training data not found at {train_path}\")\n    if not os.path.exists(test_path):\n        raise FileNotFoundError(f\"Test data not found at {test_path}\")\n    \n    train_df = pd.read_csv(train_path)\n    test_df = pd.read_csv(test_path)\n    \n    print(f\"Loaded training data: {len(train_df)} samples\")\n    print(f\"Loaded test data: {len(test_df)} samples\")\n    \n    return train_df, test_df\n\n# Load data\ntrain_df, test_df = load_competition_data()\n\n# Display basic info\nprint(\"\\nTraining data columns:\", train_df.columns.tolist())\nprint(\"Training data shape:\", train_df.shape)\nprint(\"Test data shape:\", test_df.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T13:58:37.790937Z","iopub.execute_input":"2025-08-03T13:58:37.791237Z","iopub.status.idle":"2025-08-03T13:58:37.849704Z","shell.execute_reply.started":"2025-08-03T13:58:37.791213Z","shell.execute_reply":"2025-08-03T13:58:37.848343Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Explore the data\nprint(\"First few rows of training data:\")\ndisplay(train_df.head())\n\nprint(\"\\nFirst few rows of test data:\")\ndisplay(test_df.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T13:58:37.851227Z","iopub.execute_input":"2025-08-03T13:58:37.851660Z","iopub.status.idle":"2025-08-03T13:58:37.893044Z","shell.execute_reply.started":"2025-08-03T13:58:37.851621Z","shell.execute_reply":"2025-08-03T13:58:37.891844Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Analyze missing values\nprint(\"Missing values analysis:\")\nprint(\"=\" * 50)\n\nmissing_stats = []\nfor col in CONFIG.TARGET_PROPERTIES:\n    if col in train_df.columns:\n        missing = train_df[col].isna().sum()\n        total = len(train_df)\n        percentage = (missing / total) * 100\n        missing_stats.append({\n            'Property': col,\n            'Missing': missing,\n            'Total': total,\n            'Percentage': percentage\n        })\n        print(f\"{col:>8}: {missing:>5}/{total} ({percentage:>5.1f}%)\")\n\n# Create a DataFrame for visualization\nmissing_df = pd.DataFrame(missing_stats)\n\n# Visualize missing values\nplt.figure(figsize=(12, 5))\nplt.subplot(1, 2, 1)\nsns.barplot(data=missing_df, x='Property', y='Percentage')\nplt.title('Missing Values by Property')\nplt.ylabel('Missing Percentage (%)')\nplt.xticks(rotation=45)\n\nplt.subplot(1, 2, 2)\n# Heatmap of missing values pattern\nmissing_matrix = train_df[CONFIG.TARGET_PROPERTIES].isna().astype(int)\nsns.heatmap(missing_matrix.iloc[:100], cmap='RdYlBu_r', cbar_kws={'label': 'Missing (1) / Present (0)'})\nplt.title('Missing Values Pattern (First 100 samples)')\nplt.xlabel('Properties')\nplt.ylabel('Samples')\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T13:58:37.893928Z","iopub.execute_input":"2025-08-03T13:58:37.894212Z","iopub.status.idle":"2025-08-03T13:58:38.874056Z","shell.execute_reply.started":"2025-08-03T13:58:37.894190Z","shell.execute_reply":"2025-08-03T13:58:38.873116Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Analyze property distributions\nfig, axes = plt.subplots(2, 3, figsize=(15, 10))\naxes = axes.flatten()\n\nfor i, prop in enumerate(CONFIG.TARGET_PROPERTIES):\n    if prop in train_df.columns:\n        # Remove missing values for plotting\n        values = train_df[prop].dropna()\n        \n        axes[i].hist(values, bins=50, alpha=0.7, edgecolor='black')\n        axes[i].set_title(f'{prop} Distribution\\n(n={len(values)}, mean={values.mean():.3f})')\n        axes[i].set_xlabel(prop)\n        axes[i].set_ylabel('Frequency')\n        axes[i].grid(True, alpha=0.3)\n\n# Remove empty subplot\nfig.delaxes(axes[5])\nplt.tight_layout()\nplt.show()\n\n# Summary statistics\nprint(\"\\nSummary statistics for target properties:\")\ndisplay(train_df[CONFIG.TARGET_PROPERTIES].describe())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T13:58:38.875207Z","iopub.execute_input":"2025-08-03T13:58:38.875471Z","iopub.status.idle":"2025-08-03T13:58:40.213381Z","shell.execute_reply.started":"2025-08-03T13:58:38.875451Z","shell.execute_reply":"2025-08-03T13:58:40.212453Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3. Molecular Featurization\n\nConvert SMILES strings to graph representations using RDKit.","metadata":{}},{"cell_type":"code","source":"def get_atom_features(atom):\n    \"\"\"Extract features for a single atom.\"\"\"\n    # Basic atom features\n    features = [\n        atom.GetAtomicNum(),\n        atom.GetDegree(),\n        atom.GetTotalNumHs(),\n        atom.GetTotalValence(),\n        int(atom.GetIsAromatic()),\n        int(atom.GetChiralTag()),\n        atom.GetFormalCharge(),\n        int(atom.IsInRing()),\n    ]\n    \n    # One-hot encode common atomic numbers in polymers\n    common_atoms = [1, 6, 7, 8, 9, 14, 15, 16, 17, 35, 53]  # H, C, N, O, F, Si, P, S, Cl, Br, I\n    atomic_num_one_hot = [0] * (len(common_atoms) + 1)  # +1 for \"other\"\n    \n    atomic_num = atom.GetAtomicNum()\n    if atomic_num in common_atoms:\n        atomic_num_one_hot[common_atoms.index(atomic_num)] = 1\n    else:\n        atomic_num_one_hot[-1] = 1  # \"other\" category\n    \n    # Hybridization one-hot encoding\n    hybridization_types = [\n        rdchem.HybridizationType.SP,\n        rdchem.HybridizationType.SP2,\n        rdchem.HybridizationType.SP3,\n        rdchem.HybridizationType.SP3D,\n        rdchem.HybridizationType.SP3D2,\n    ]\n    hybridization_one_hot = [0] * (len(hybridization_types) + 1)  # +1 for \"other\"\n    \n    hybridization = atom.GetHybridization()\n    if hybridization in hybridization_types:\n        hybridization_one_hot[hybridization_types.index(hybridization)] = 1\n    else:\n        hybridization_one_hot[-1] = 1\n    \n    return features + atomic_num_one_hot + hybridization_one_hot\n\n\ndef get_bond_features(bond):\n    \"\"\"Extract features for a single bond.\"\"\"\n    # Bond type one-hot encoding\n    bond_types = [\n        Chem.rdchem.BondType.SINGLE,\n        Chem.rdchem.BondType.DOUBLE,\n        Chem.rdchem.BondType.TRIPLE,\n        Chem.rdchem.BondType.AROMATIC,\n    ]\n    bond_type_one_hot = [0] * (len(bond_types) + 1)  # +1 for \"other\"\n    \n    bond_type = bond.GetBondType()\n    if bond_type in bond_types:\n        bond_type_one_hot[bond_types.index(bond_type)] = 1\n    else:\n        bond_type_one_hot[-1] = 1\n    \n    # Additional bond features\n    features = [\n        int(bond.IsInRing()),\n        int(bond.GetIsConjugated()),\n    ]\n    \n    return features + bond_type_one_hot\n\n\ndef smiles_to_graph(smiles_string: str):\n    \"\"\"Convert a SMILES string to a PyG Data object.\"\"\"\n    mol = Chem.MolFromSmiles(smiles_string)\n    if mol is None:\n        return None\n    \n    mol = Chem.AddHs(mol)  # Add explicit hydrogens\n\n    # Get atom features\n    atom_features = [get_atom_features(atom) for atom in mol.GetAtoms()]\n    x = torch.tensor(atom_features, dtype=torch.float)\n\n    # Get bond features and connectivity\n    if mol.GetNumBonds() > 0:\n        edge_indices = []\n        edge_attrs = []\n        for bond in mol.GetBonds():\n            i = bond.GetBeginAtomIdx()\n            j = bond.GetEndAtomIdx()\n            edge_indices.append((i, j))\n            edge_indices.append((j, i))  # Graph must be undirected\n\n            # Enhanced bond features\n            bond_features = get_bond_features(bond)\n            edge_attrs.append(bond_features)\n            edge_attrs.append(bond_features)  # Same features for both directions\n\n        edge_index = torch.tensor(edge_indices, dtype=torch.long).t().contiguous()\n        edge_attr = torch.tensor(edge_attrs, dtype=torch.float)\n    else:\n        # Handle molecules with no bonds (e.g., single atoms)\n        edge_index = torch.empty((2, 0), dtype=torch.long)\n        edge_attr = torch.empty((0, 7), dtype=torch.float)  # 7 bond features\n\n    # Create the PyG Data object\n    data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr)\n    \n    # Store number of features for model instantiation later\n    data.num_atom_features = x.size(1)\n\n    return data\n\nprint(\"Molecular featurization functions defined!\")\nprint(f\"Expected atom feature size: {len(get_atom_features(Chem.MolFromSmiles('C').GetAtomWithIdx(0)))}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T13:58:40.214251Z","iopub.execute_input":"2025-08-03T13:58:40.214527Z","iopub.status.idle":"2025-08-03T13:58:40.231969Z","shell.execute_reply.started":"2025-08-03T13:58:40.214504Z","shell.execute_reply":"2025-08-03T13:58:40.231122Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Test featurization on a sample SMILES\nsample_smiles = train_df['SMILES'].iloc[0]\nprint(f\"Sample SMILES: {sample_smiles}\")\n\nsample_graph = smiles_to_graph(sample_smiles)\nif sample_graph is not None:\n    print(f\"Graph created successfully!\")\n    print(f\"  Number of atoms: {sample_graph.x.size(0)}\")\n    print(f\"  Atom feature size: {sample_graph.x.size(1)}\")\n    print(f\"  Number of bonds: {sample_graph.edge_index.size(1) // 2}\")\n    print(f\"  Edge feature size: {sample_graph.edge_attr.size(1) if sample_graph.edge_attr.size(0) > 0 else 0}\")\nelse:\n    print(\"Failed to create graph from SMILES\")\n\n# Test on a few more samples to check for failures\nprint(\"\\nTesting featurization on first 10 SMILES...\")\nsuccess_count = 0\nfor i, smiles in enumerate(train_df['SMILES'].head(10)):\n    graph = smiles_to_graph(smiles)\n    if graph is not None:\n        success_count += 1\n    else:\n        print(f\"Failed on sample {i}: {smiles}\")\n\nprint(f\"Success rate: {success_count}/10\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T13:58:40.232987Z","iopub.execute_input":"2025-08-03T13:58:40.233242Z","iopub.status.idle":"2025-08-03T13:58:40.301684Z","shell.execute_reply.started":"2025-08-03T13:58:40.233220Z","shell.execute_reply":"2025-08-03T13:58:40.300534Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 4. Dataset Implementation\n\nCreate a PyTorch Geometric dataset that handles multi-target prediction and missing values.","metadata":{}},{"cell_type":"code","source":"class PolymerDataset(Dataset):\n    \"\"\"PyTorch Geometric Dataset for polymer data with multi-target prediction.\"\"\"\n\n    def __init__(self, df: pd.DataFrame, target_cols: Optional[List[str]] = None, is_test: bool = False):\n        super().__init__()\n        self.df = df\n        self.is_test = is_test\n        \n        # Default target columns for the competition\n        if target_cols is None:\n            target_cols = CONFIG.TARGET_PROPERTIES\n        self.target_cols = target_cols\n        \n        self.smiles_list = df['SMILES'].tolist()\n        self.ids = df['id'].tolist()\n        \n        if not is_test:\n            # Extract targets and create masks for missing values\n            self.targets = []\n            self.masks = []\n            \n            for idx in range(len(df)):\n                target_values = []\n                mask_values = []\n                \n                for col in target_cols:\n                    if col in df.columns:\n                        val = df.iloc[idx][col]\n                        if pd.isna(val):\n                            target_values.append(0.0)  # Placeholder for missing values\n                            mask_values.append(0.0)    # Mask indicates missing\n                        else:\n                            target_values.append(float(val))\n                            mask_values.append(1.0)    # Mask indicates present\n                    else:\n                        target_values.append(0.0)\n                        mask_values.append(0.0)\n                \n                self.targets.append(target_values)\n                self.masks.append(mask_values)\n        \n        self.cache = {}  # Cache graphs to avoid re-computing\n\n    def len(self):\n        return len(self.df)\n\n    def get(self, idx):\n        if idx in self.cache:\n            data = self.cache[idx]\n        else:\n            smiles = self.smiles_list[idx]\n            data = smiles_to_graph(smiles)\n            if data is None:  # Handle RDKit parsing errors\n                return None\n            self.cache[idx] = data\n\n        # Add ID (store as integer, not tensor)\n        data.id = int(self.ids[idx])\n        \n        if not self.is_test:\n            # Add target values and masks - reshape to (1, 5) to preserve structure during batching\n            data.y = torch.tensor(self.targets[idx], dtype=torch.float).unsqueeze(0)  # Shape: (1, 5)\n            data.mask = torch.tensor(self.masks[idx], dtype=torch.float).unsqueeze(0)  # Shape: (1, 5)\n        \n        return data\n\nprint(\"PolymerDataset class defined!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T13:58:40.305013Z","iopub.execute_input":"2025-08-03T13:58:40.305316Z","iopub.status.idle":"2025-08-03T13:58:40.317831Z","shell.execute_reply.started":"2025-08-03T13:58:40.305291Z","shell.execute_reply":"2025-08-03T13:58:40.316740Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Test the dataset\nprint(\"Testing dataset creation...\")\n\n# Create a small test dataset\ntest_dataset = PolymerDataset(train_df.head(10), is_test=False)\nprint(f\"Dataset length: {len(test_dataset)}\")\n\n# Test a single sample\nsample = test_dataset.get(0)\nif sample is not None:\n    print(f\"Sample successful:\")\n    print(f\"  ID: {sample.id}\")\n    print(f\"  Node features shape: {sample.x.shape}\")\n    print(f\"  Edge index shape: {sample.edge_index.shape}\")\n    print(f\"  Targets shape: {sample.y.shape}\")\n    print(f\"  Mask shape: {sample.mask.shape}\")\n    print(f\"  Targets: {sample.y.squeeze()}\")\n    print(f\"  Mask: {sample.mask.squeeze()}\")\nelse:\n    print(\"Sample failed\")\n\n# Test batching\nprint(\"\\nTesting data loader...\")\nloader = DataLoader(test_dataset, batch_size=4, shuffle=False)\nfor batch in loader:\n    print(f\"Batch:\")\n    print(f\"  Batch size: {batch.num_graphs}\")\n    print(f\"  Node features shape: {batch.x.shape}\")\n    print(f\"  Targets shape: {batch.y.shape}\")\n    print(f\"  Mask shape: {batch.mask.shape}\")\n    break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T13:58:40.319148Z","iopub.execute_input":"2025-08-03T13:58:40.319490Z","iopub.status.idle":"2025-08-03T13:58:40.414420Z","shell.execute_reply.started":"2025-08-03T13:58:40.319468Z","shell.execute_reply":"2025-08-03T13:58:40.413414Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 5. Model Architecture\n\nImplement a Graph Convolutional Network for multi-target polymer property prediction.","metadata":{}},{"cell_type":"code","source":"class PolymerGCN(nn.Module):\n    \"\"\"Graph Convolutional Network for polymer property prediction.\"\"\"\n\n    def __init__(self, num_atom_features: int, hidden_channels: int, num_gcn_layers: int):\n        super().__init__()\n        self.convs = nn.ModuleList()\n        self.bns = nn.ModuleList()\n\n        # Input layer\n        self.convs.append(GCNConv(num_atom_features, hidden_channels))\n        self.bns.append(nn.BatchNorm1d(hidden_channels))\n\n        # Hidden layers\n        for _ in range(num_gcn_layers - 1):\n            self.convs.append(GCNConv(hidden_channels, hidden_channels))\n            self.bns.append(nn.BatchNorm1d(hidden_channels))\n\n        # Output layer for multi-target prediction (5 properties)\n        self.out = nn.Sequential(\n            nn.Linear(hidden_channels, hidden_channels // 2),\n            nn.ReLU(),\n            nn.Linear(hidden_channels // 2, 5)  # 5 properties: Tg, FFV, Tc, Density, Rg\n        )\n\n    def forward(self, data):\n        \"\"\"Forward pass through the network.\"\"\"\n        x, edge_index, batch = data.x, data.edge_index, data.batch\n        \n        # Message passing through GCN layers\n        for conv, bn in zip(self.convs, self.bns):\n            x = conv(x, edge_index)\n            x = bn(x)\n            x = F.relu(x)\n\n        # Readout phase: Aggregate node features into a single graph-level representation\n        x = global_mean_pool(x, batch)\n        \n        # Final prediction\n        return self.out(x)\n\nprint(\"PolymerGCN model class defined!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T13:58:40.415430Z","iopub.execute_input":"2025-08-03T13:58:40.415767Z","iopub.status.idle":"2025-08-03T13:58:40.426718Z","shell.execute_reply.started":"2025-08-03T13:58:40.415744Z","shell.execute_reply":"2025-08-03T13:58:40.425550Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Test model creation\nprint(\"Testing model creation...\")\n\n# Get feature dimensions from a sample\nsample_data = test_dataset.get(0)\nnum_atom_features = sample_data.num_atom_features\n\n# Create model\nmodel = PolymerGCN(\n    num_atom_features=num_atom_features,\n    hidden_channels=CONFIG.HIDDEN_CHANNELS,\n    num_gcn_layers=CONFIG.NUM_GCN_LAYERS\n).to(CONFIG.DEVICE)\n\nprint(f\"Model created successfully!\")\nprint(f\"  Input features: {num_atom_features}\")\nprint(f\"  Hidden channels: {CONFIG.HIDDEN_CHANNELS}\")\nprint(f\"  GCN layers: {CONFIG.NUM_GCN_LAYERS}\")\nprint(f\"  Output targets: 5\")\nprint(f\"  Device: {CONFIG.DEVICE}\")\n\n# Count parameters\ntotal_params = sum(p.numel() for p in model.parameters())\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(f\"  Total parameters: {total_params:,}\")\nprint(f\"  Trainable parameters: {trainable_params:,}\")\n\n# Test forward pass\nmodel.eval()\nwith torch.no_grad():\n    # Create a small batch\n    loader = DataLoader(test_dataset, batch_size=2, shuffle=False)\n    batch = next(iter(loader))\n    batch = batch.to(CONFIG.DEVICE)\n    \n    output = model(batch)\n    print(f\"\\nForward pass test:\")\n    print(f\"  Input batch size: {batch.num_graphs}\")\n    print(f\"  Output shape: {output.shape}\")\n    print(f\"  Output sample: {output[0].cpu().numpy()}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T13:58:40.427808Z","iopub.execute_input":"2025-08-03T13:58:40.428237Z","iopub.status.idle":"2025-08-03T13:58:40.561225Z","shell.execute_reply.started":"2025-08-03T13:58:40.428204Z","shell.execute_reply":"2025-08-03T13:58:40.559938Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 6. Training Implementation\n\nImplement training functions that handle missing values and multi-target prediction.","metadata":{}},{"cell_type":"code","source":"def masked_mse_loss(predictions, targets, masks):\n    \"\"\"Calculate MSE loss only for non-missing values.\"\"\"\n    # Ensure all tensors have the same shape\n    assert predictions.shape == targets.shape == masks.shape, f\"Shape mismatch: pred {predictions.shape}, target {targets.shape}, mask {masks.shape}\"\n    \n    # Only compute loss for non-missing values\n    masked_predictions = predictions * masks\n    masked_targets = targets * masks\n    \n    # Calculate squared differences\n    squared_diff = (masked_predictions - masked_targets) ** 2\n    \n    # Sum over all dimensions and divide by number of non-missing values\n    total_loss = torch.sum(squared_diff)\n    total_count = torch.sum(masks)\n    \n    if total_count > 0:\n        return total_loss / total_count\n    else:\n        return torch.tensor(0.0, device=predictions.device)\n\n\ndef train_one_epoch(model, loader, optimizer, device):\n    \"\"\"Perform one full training pass over the dataset.\"\"\"\n    model.train()\n    total_loss = 0\n    total_samples = 0\n    \n    for data in tqdm(loader, desc=\"Training\", leave=False):\n        data = data.to(device)\n        optimizer.zero_grad()\n        \n        out = model(data)\n        loss = masked_mse_loss(out, data.y, data.mask)\n        \n        loss.backward()\n        optimizer.step()\n        \n        total_loss += loss.item() * data.num_graphs\n        total_samples += data.num_graphs\n        \n    return total_loss / total_samples\n\n\n@torch.no_grad()\ndef evaluate(model, loader, device):\n    \"\"\"Evaluate the model on a dataset.\"\"\"\n    model.eval()\n    total_loss = 0\n    total_samples = 0\n    \n    all_preds = []\n    all_targets = []\n    all_masks = []\n    \n    for data in tqdm(loader, desc=\"Evaluating\", leave=False):\n        data = data.to(device)\n        out = model(data)\n        \n        loss = masked_mse_loss(out, data.y, data.mask)\n        total_loss += loss.item() * data.num_graphs\n        total_samples += data.num_graphs\n        \n        all_preds.append(out.cpu())\n        all_targets.append(data.y.cpu())\n        all_masks.append(data.mask.cpu())\n\n    avg_loss = total_loss / total_samples\n    \n    # Calculate per-property RMSE\n    preds = torch.cat(all_preds, dim=0)  # Shape: (N, 5)\n    targets = torch.cat(all_targets, dim=0)  # Shape: (N, 5)\n    masks = torch.cat(all_masks, dim=0)  # Shape: (N, 5)\n    \n    property_names = CONFIG.TARGET_PROPERTIES\n    rmses = {}\n    \n    for i, prop_name in enumerate(property_names):\n        prop_mask = masks[:, i]\n        if prop_mask.sum() > 0:  # Only calculate if we have non-missing values\n            prop_preds = preds[:, i][prop_mask == 1]\n            prop_targets = targets[:, i][prop_mask == 1]\n            rmse = torch.sqrt(torch.mean((prop_preds - prop_targets) ** 2))\n            rmses[prop_name] = rmse.item()\n        else:\n            rmses[prop_name] = float('nan')\n    \n    return avg_loss, rmses, (preds.numpy(), targets.numpy(), masks.numpy())\n\n\n@torch.no_grad()\ndef predict(model, loader, device):\n    \"\"\"Generate predictions for test data.\"\"\"\n    model.eval()\n    all_ids = []\n    all_preds = []\n    \n    for data in tqdm(loader, desc=\"Predicting\", leave=False):\n        data = data.to(device)\n        out = model(data)\n        \n        all_ids.extend(data.id)\n        all_preds.append(out.cpu())\n    \n    predictions = torch.cat(all_preds, dim=0).numpy()\n    \n    return all_ids, predictions\n\nprint(\"Training functions defined!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T13:58:40.562387Z","iopub.execute_input":"2025-08-03T13:58:40.562704Z","iopub.status.idle":"2025-08-03T13:58:40.583588Z","shell.execute_reply.started":"2025-08-03T13:58:40.562669Z","shell.execute_reply":"2025-08-03T13:58:40.582345Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 7. Competition Metrics\n\nImplement the competition's weighted MAE metric.","metadata":{}},{"cell_type":"code","source":"def weighted_mae(predictions, targets, masks=None):\n    \"\"\"Calculate the weighted Mean Absolute Error (wMAE) as defined in the competition.\"\"\"\n    # Convert to numpy if needed\n    if torch.is_tensor(predictions):\n        predictions = predictions.detach().cpu().numpy()\n    if torch.is_tensor(targets):\n        targets = targets.detach().cpu().numpy()\n    if masks is not None and torch.is_tensor(masks):\n        masks = masks.detach().cpu().numpy()\n    \n    # If no masks provided, assume all values are present\n    if masks is None:\n        masks = np.ones_like(predictions)\n    \n    # Calculate weights for each property\n    weights = []\n    total_samples = predictions.shape[0]\n    \n    for j in range(5):  # 5 properties\n        # Count non-missing values for this property\n        N_j = np.sum(masks[:, j])\n        \n        if N_j == 0:\n            weights.append(0.0)\n            continue\n        \n        # Calculate range R_j for this property (using only non-missing values)\n        valid_mask = masks[:, j] == 1\n        if np.sum(valid_mask) > 1:\n            property_values = targets[valid_mask, j]\n            R_j = np.max(property_values) - np.min(property_values)\n            if R_j == 0:\n                R_j = 1.0  # Avoid division by zero\n        else:\n            R_j = 1.0\n        \n        # Calculate weight: w_j = (5 / sqrt(N_j)) / R_j\n        w_j = (5.0 / np.sqrt(N_j)) / R_j\n        weights.append(w_j)\n    \n    # Normalize weights so they sum to 5\n    weights = np.array(weights)\n    if np.sum(weights) > 0:\n        weights = weights * (5.0 / np.sum(weights))\n    \n    # Calculate weighted MAE\n    total_weighted_error = 0.0\n    total_count = 0\n    \n    for i in range(total_samples):\n        for j in range(5):\n            if masks[i, j] == 1:  # Only consider non-missing values\n                error = abs(predictions[i, j] - targets[i, j])\n                weighted_error = weights[j] * error\n                total_weighted_error += weighted_error\n                total_count += 1\n    \n    if total_count == 0:\n        return float('inf')\n    \n    wmae = total_weighted_error / total_count\n    return wmae\n\n\ndef print_competition_metrics(predictions, targets, masks=None):\n    \"\"\"Print comprehensive metrics in a nice format.\"\"\"\n    wmae = weighted_mae(predictions, targets, masks)\n    \n    print(\"\\n\" + \"=\"*60)\n    print(\"COMPETITION METRICS\")\n    print(\"=\"*60)\n    print(f\"Weighted MAE (Competition Metric): {wmae:.6f}\")\n    \n    # Per-property metrics\n    if masks is None:\n        masks = np.ones_like(predictions)\n    \n    print(\"\\nPer-Property Metrics:\")\n    print(\"-\"*60)\n    print(f\"{'Property':<10} {'Count':<8} {'MAE':<12} {'RMSE':<12} {'Range':<12}\")\n    print(\"-\"*60)\n    \n    for j, prop_name in enumerate(CONFIG.TARGET_PROPERTIES):\n        valid_mask = masks[:, j] == 1\n        count = np.sum(valid_mask)\n        \n        if count > 0:\n            pred_vals = predictions[valid_mask, j]\n            true_vals = targets[valid_mask, j]\n            \n            mae = np.mean(np.abs(pred_vals - true_vals))\n            rmse = np.sqrt(np.mean((pred_vals - true_vals) ** 2))\n            prop_range = np.max(true_vals) - np.min(true_vals) if len(true_vals) > 1 else 0.0\n            \n            print(f\"{prop_name:<10} {count:<8} {mae:<12.6f} {rmse:<12.6f} {prop_range:<12.6f}\")\n        else:\n            print(f\"{prop_name:<10} {count:<8} {'N/A':<12} {'N/A':<12} {'N/A':<12}\")\n    \n    print(\"=\"*60)\n\nprint(\"Competition metrics functions defined!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T13:58:40.584388Z","iopub.execute_input":"2025-08-03T13:58:40.584636Z","iopub.status.idle":"2025-08-03T13:58:40.619529Z","shell.execute_reply.started":"2025-08-03T13:58:40.584617Z","shell.execute_reply":"2025-08-03T13:58:40.618530Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 8. Data Preparation and Training Loop\n\nPrepare the datasets and train the model.","metadata":{}},{"cell_type":"code","source":"# Prepare datasets\ndef prepare_datasets(train_df, test_df, val_split=0.2):\n    \"\"\"Prepare datasets for training, validation, and testing.\"\"\"\n    # Create datasets\n    full_train_dataset = PolymerDataset(train_df, is_test=False)\n    test_dataset = PolymerDataset(test_df, is_test=True)\n    \n    # Filter out invalid SMILES from training data\n    valid_indices = []\n    print(\"Filtering invalid SMILES...\")\n    for i in tqdm(range(len(full_train_dataset))):\n        data = full_train_dataset.get(i)\n        if data is not None:\n            valid_indices.append(i)\n    \n    if len(valid_indices) != len(full_train_dataset):\n        print(f\"Warning: Filtered out {len(full_train_dataset) - len(valid_indices)} invalid SMILES from training data.\")\n    \n    # Create clean training dataset\n    train_df_clean = train_df.iloc[valid_indices].reset_index(drop=True)\n    clean_train_dataset = PolymerDataset(train_df_clean, is_test=False)\n    \n    # Split training data into train and validation\n    train_size = int((1.0 - val_split) * len(clean_train_dataset))\n    val_size = len(clean_train_dataset) - train_size\n    train_dataset, val_dataset = random_split(clean_train_dataset, [train_size, val_size])\n    \n    print(f\"\\nDataset sizes:\")\n    print(f\"  Training: {len(train_dataset)}\")\n    print(f\"  Validation: {len(val_dataset)}\")\n    print(f\"  Test: {len(test_dataset)}\")\n    \n    return train_dataset, val_dataset, test_dataset\n\n# Prepare datasets\ntrain_dataset, val_dataset, test_dataset = prepare_datasets(\n    train_df, test_df, val_split=CONFIG.VAL_SPLIT_FRACTION\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T13:58:40.620603Z","iopub.execute_input":"2025-08-03T13:58:40.620961Z","iopub.status.idle":"2025-08-03T13:59:01.381873Z","shell.execute_reply.started":"2025-08-03T13:58:40.620936Z","shell.execute_reply":"2025-08-03T13:59:01.380710Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create data loaders\ntrain_loader = DataLoader(train_dataset, batch_size=CONFIG.BATCH_SIZE, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=CONFIG.BATCH_SIZE, shuffle=False)\ntest_loader = DataLoader(test_dataset, batch_size=CONFIG.BATCH_SIZE, shuffle=False)\n\nprint(f\"Data loaders created:\")\nprint(f\"  Train batches: {len(train_loader)}\")\nprint(f\"  Val batches: {len(val_loader)}\")\nprint(f\"  Test batches: {len(test_loader)}\")\n\n# Get feature dimensions\nfirst_data = train_dataset[0]\nnum_atom_features = first_data.num_atom_features\nprint(f\"  Atom features: {num_atom_features}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T13:59:01.383140Z","iopub.execute_input":"2025-08-03T13:59:01.383616Z","iopub.status.idle":"2025-08-03T13:59:01.395380Z","shell.execute_reply.started":"2025-08-03T13:59:01.383585Z","shell.execute_reply":"2025-08-03T13:59:01.394083Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Initialize model, optimizer, and scheduler\nmodel = PolymerGCN(\n    num_atom_features=num_atom_features,\n    hidden_channels=CONFIG.HIDDEN_CHANNELS,\n    num_gcn_layers=CONFIG.NUM_GCN_LAYERS\n).to(CONFIG.DEVICE)\n\noptimizer = torch.optim.Adam(\n    model.parameters(),\n    lr=CONFIG.LEARNING_RATE,\n    weight_decay=CONFIG.WEIGHT_DECAY\n)\n\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n    optimizer, mode='min', factor=0.5, patience=5\n)\n\nprint(f\"Model initialized:\")\nprint(f\"  Parameters: {sum(p.numel() for p in model.parameters()):,}\")\nprint(f\"  Device: {CONFIG.DEVICE}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T13:59:01.396586Z","iopub.execute_input":"2025-08-03T13:59:01.397381Z","iopub.status.idle":"2025-08-03T13:59:01.438005Z","shell.execute_reply.started":"2025-08-03T13:59:01.397354Z","shell.execute_reply":"2025-08-03T13:59:01.437055Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Training loop\nprint(f\"Starting training for {CONFIG.NUM_EPOCHS} epochs...\")\nprint(\"=\"*80)\n\n# Training history\nhistory = {\n    'train_losses': [],\n    'val_losses': [],\n    'val_rmses': [],\n    'learning_rates': [],\n    'wmae_scores': []\n}\n\nbest_val_loss = float('inf')\nbest_wmae = float('inf')\npatience_counter = 0\nearly_stopping_patience = 15\n\nfor epoch in range(1, CONFIG.NUM_EPOCHS + 1):\n    # Train\n    train_loss = train_one_epoch(model, train_loader, optimizer, CONFIG.DEVICE)\n    history['train_losses'].append(train_loss)\n    \n    # Validate\n    val_loss, val_rmses, (val_preds, val_targets, val_masks) = evaluate(model, val_loader, CONFIG.DEVICE)\n    history['val_losses'].append(val_loss)\n    history['val_rmses'].append(val_rmses)\n    history['learning_rates'].append(optimizer.param_groups[0]['lr'])\n    \n    # Calculate competition metric\n    wmae = weighted_mae(val_preds, val_targets, val_masks)\n    history['wmae_scores'].append(wmae)\n    \n    # Update learning rate\n    scheduler.step(val_loss)\n    \n    # Print progress\n    rmse_str = \", \".join([f\"{k}: {v:.4f}\" for k, v in val_rmses.items() if not np.isnan(v)])\n    print(f\"Epoch {epoch:3d} | Train: {train_loss:.4f} | Val: {val_loss:.4f} | wMAE: {wmae:.6f} | LR: {optimizer.param_groups[0]['lr']:.6f}\")\n    print(f\"         RMSE: {rmse_str}\")\n    \n    # Save best model\n    if wmae < best_wmae:\n        best_wmae = wmae\n        best_val_loss = val_loss\n        patience_counter = 0\n        \n        # Save model state\n        torch.save({\n            'epoch': epoch,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'val_loss': val_loss,\n            'wmae': wmae,\n            'val_rmses': val_rmses,\n        }, 'best_model.pt')\n        \n        print(f\"         -> New best wMAE! Model saved.\")\n    else:\n        patience_counter += 1\n    \n    # Early stopping\n    if patience_counter >= early_stopping_patience:\n        print(f\"\\nEarly stopping triggered after {patience_counter} epochs without improvement.\")\n        break\n    \n    print(\"-\" * 80)\n\nprint(f\"\\nTraining complete!\")\nprint(f\"Best validation wMAE: {best_wmae:.6f}\")\nprint(f\"Best validation loss: {best_val_loss:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T13:59:01.438960Z","iopub.execute_input":"2025-08-03T13:59:01.439405Z","iopub.status.idle":"2025-08-03T14:13:28.003975Z","shell.execute_reply.started":"2025-08-03T13:59:01.439380Z","shell.execute_reply":"2025-08-03T14:13:28.002966Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plot training history\nfig, axes = plt.subplots(2, 2, figsize=(15, 10))\n\n# Training and validation loss\naxes[0, 0].plot(history['train_losses'], label='Train Loss', alpha=0.8)\naxes[0, 0].plot(history['val_losses'], label='Val Loss', alpha=0.8)\naxes[0, 0].set_title('Training and Validation Loss')\naxes[0, 0].set_xlabel('Epoch')\naxes[0, 0].set_ylabel('Loss')\naxes[0, 0].legend()\naxes[0, 0].grid(True, alpha=0.3)\n\n# Competition metric (wMAE)\naxes[0, 1].plot(history['wmae_scores'], label='wMAE', color='red', alpha=0.8)\naxes[0, 1].set_title('Competition Metric (wMAE)')\naxes[0, 1].set_xlabel('Epoch')\naxes[0, 1].set_ylabel('wMAE')\naxes[0, 1].legend()\naxes[0, 1].grid(True, alpha=0.3)\n\n# Learning rate\naxes[1, 0].plot(history['learning_rates'], label='Learning Rate', color='green', alpha=0.8)\naxes[1, 0].set_title('Learning Rate Schedule')\naxes[1, 0].set_xlabel('Epoch')\naxes[1, 0].set_ylabel('Learning Rate')\naxes[1, 0].set_yscale('log')\naxes[1, 0].legend()\naxes[1, 0].grid(True, alpha=0.3)\n\n# RMSE by property\nfor prop in CONFIG.TARGET_PROPERTIES:\n    rmse_values = [rmse_dict.get(prop, np.nan) for rmse_dict in history['val_rmses']]\n    # Only plot if we have valid values\n    if not all(np.isnan(rmse_values)):\n        axes[1, 1].plot(rmse_values, label=prop, alpha=0.8)\n\naxes[1, 1].set_title('Validation RMSE by Property')\naxes[1, 1].set_xlabel('Epoch')\naxes[1, 1].set_ylabel('RMSE')\naxes[1, 1].legend()\naxes[1, 1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T14:13:28.004998Z","iopub.execute_input":"2025-08-03T14:13:28.005282Z","iopub.status.idle":"2025-08-03T14:13:29.207999Z","shell.execute_reply.started":"2025-08-03T14:13:28.005260Z","shell.execute_reply":"2025-08-03T14:13:29.206846Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 9. Evaluation and Submission\n\nLoad the best model and generate predictions for the test set.","metadata":{}},{"cell_type":"code","source":"# Load the best model\nprint(\"Loading best model...\")\ncheckpoint = torch.load('best_model.pt', map_location=CONFIG.DEVICE, weights_only=False)\nmodel.load_state_dict(checkpoint['model_state_dict'])\n\nprint(f\"Best model loaded:\")\nprint(f\"  Epoch: {checkpoint['epoch']}\")\nprint(f\"  Validation Loss: {checkpoint['val_loss']:.4f}\")\nprint(f\"  Competition wMAE: {checkpoint['wmae']:.6f}\")\n\n# Final evaluation on validation set\nprint(\"\\nFinal validation evaluation:\")\nval_loss, val_rmses, (val_preds, val_targets, val_masks) = evaluate(model, val_loader, CONFIG.DEVICE)\nprint_competition_metrics(val_preds, val_targets, val_masks)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T14:13:29.209119Z","iopub.execute_input":"2025-08-03T14:13:29.209405Z","iopub.status.idle":"2025-08-03T14:13:30.308506Z","shell.execute_reply.started":"2025-08-03T14:13:29.209383Z","shell.execute_reply":"2025-08-03T14:13:30.307512Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Generate predictions for test set\nprint(\"Generating predictions for test set...\")\ntest_ids, test_predictions = predict(model, test_loader, CONFIG.DEVICE)\n\n# Create submission DataFrame\nsubmission_df = pd.DataFrame({\n    'id': [int(id_val) if hasattr(id_val, 'item') else int(id_val) for id_val in test_ids],\n    'Tg': test_predictions[:, 0],\n    'FFV': test_predictions[:, 1],\n    'Tc': test_predictions[:, 2],\n    'Density': test_predictions[:, 3],\n    'Rg': test_predictions[:, 4]\n})\n\n# Save submission\nsubmission_filename = f\"submission.csv\"\nsubmission_df.to_csv(submission_filename, index=False)\n\nprint(f\"\\nSubmission saved to: {submission_filename}\")\nprint(f\"Submission shape: {submission_df.shape}\")\nprint(\"\\nSample predictions:\")\ndisplay(submission_df.head())\n\n# Verify submission format\nexpected_columns = ['id', 'Tg', 'FFV', 'Tc', 'Density', 'Rg']\nif list(submission_df.columns) == expected_columns:\n    print(\"✅ Submission format is correct!\")\nelse:\n    print(\"❌ Submission format is incorrect!\")\n    print(f\"Expected: {expected_columns}\")\n    print(f\"Got: {list(submission_df.columns)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T14:13:30.309598Z","iopub.execute_input":"2025-08-03T14:13:30.309959Z","iopub.status.idle":"2025-08-03T14:13:30.362956Z","shell.execute_reply.started":"2025-08-03T14:13:30.309929Z","shell.execute_reply":"2025-08-03T14:13:30.362013Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 10. Results and Analysis\n\nAnalyze the model performance and predictions.","metadata":{}},{"cell_type":"code","source":"# Analyze prediction distributions\nfig, axes = plt.subplots(2, 3, figsize=(18, 12))\naxes = axes.flatten()\n\nfor i, prop in enumerate(CONFIG.TARGET_PROPERTIES):\n    # Get validation predictions and targets for this property\n    prop_mask = val_masks[:, i] == 1\n    \n    if np.sum(prop_mask) > 0:\n        pred_vals = val_preds[prop_mask, i]\n        true_vals = val_targets[prop_mask, i]\n        \n        # Scatter plot: predictions vs targets\n        axes[i].scatter(true_vals, pred_vals, alpha=0.6, s=20)\n        \n        # Perfect prediction line\n        min_val = min(true_vals.min(), pred_vals.min())\n        max_val = max(true_vals.max(), pred_vals.max())\n        axes[i].plot([min_val, max_val], [min_val, max_val], 'r--', alpha=0.8, label='Perfect Prediction')\n        \n        # Calculate R²\n        r2 = 1 - np.sum((true_vals - pred_vals)**2) / np.sum((true_vals - np.mean(true_vals))**2)\n        \n        axes[i].set_title(f'{prop} (n={len(pred_vals)}, R²={r2:.3f})')\n        axes[i].set_xlabel('True Values')\n        axes[i].set_ylabel('Predicted Values')\n        axes[i].legend()\n        axes[i].grid(True, alpha=0.3)\n    else:\n        axes[i].text(0.5, 0.5, f'No data for {prop}', \n                    horizontalalignment='center', verticalalignment='center',\n                    transform=axes[i].transAxes, fontsize=14)\n        axes[i].set_title(f'{prop} (No Data)')\n\n# Remove empty subplot\nfig.delaxes(axes[5])\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T14:13:30.364050Z","iopub.execute_input":"2025-08-03T14:13:30.364339Z","iopub.status.idle":"2025-08-03T14:13:31.629525Z","shell.execute_reply.started":"2025-08-03T14:13:30.364311Z","shell.execute_reply":"2025-08-03T14:13:31.628493Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Analyze test predictions\nprint(\"Test Prediction Statistics:\")\nprint(\"=\" * 50)\n\ntest_stats = submission_df[CONFIG.TARGET_PROPERTIES].describe()\ndisplay(test_stats)\n\n# Plot test prediction distributions\nfig, axes = plt.subplots(2, 3, figsize=(15, 10))\naxes = axes.flatten()\n\nfor i, prop in enumerate(CONFIG.TARGET_PROPERTIES):\n    values = submission_df[prop]\n    \n    axes[i].hist(values, bins=20, alpha=0.7, edgecolor='black')\n    axes[i].set_title(f'{prop} Test Predictions\\n(mean={values.mean():.3f}, std={values.std():.3f})')\n    axes[i].set_xlabel(prop)\n    axes[i].set_ylabel('Frequency')\n    axes[i].grid(True, alpha=0.3)\n\n# Remove empty subplot\nfig.delaxes(axes[5])\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T14:13:31.630655Z","iopub.execute_input":"2025-08-03T14:13:31.631061Z","iopub.status.idle":"2025-08-03T14:13:32.783010Z","shell.execute_reply.started":"2025-08-03T14:13:31.631033Z","shell.execute_reply":"2025-08-03T14:13:32.781710Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Summary\nprint(\"\\n\" + \"=\"*80)\nprint(\"NEURIPS OPEN POLYMER PREDICTION 2025 - SOLUTION SUMMARY\")\nprint(\"=\"*80)\nprint(f\"Model Architecture: Graph Convolutional Network\")\nprint(f\"  - Input features: {num_atom_features} atom features\")\nprint(f\"  - Hidden channels: {CONFIG.HIDDEN_CHANNELS}\")\nprint(f\"  - GCN layers: {CONFIG.NUM_GCN_LAYERS}\")\nprint(f\"  - Output: 5 polymer properties\")\nprint(f\"  - Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n\nprint(f\"\\nTraining Configuration:\")\nprint(f\"  - Training samples: {len(train_dataset):,}\")\nprint(f\"  - Validation samples: {len(val_dataset):,}\")\nprint(f\"  - Test samples: {len(test_dataset):,}\")\nprint(f\"  - Batch size: {CONFIG.BATCH_SIZE}\")\nprint(f\"  - Learning rate: {CONFIG.LEARNING_RATE}\")\nprint(f\"  - Epochs trained: {len(history['train_losses'])}\")\n\nprint(f\"\\nBest Performance:\")\nprint(f\"  - Validation Loss: {checkpoint['val_loss']:.4f}\")\nprint(f\"  - Competition wMAE: {checkpoint['wmae']:.6f}\")\n\nprint(f\"\\nSubmission:\")\nprint(f\"  - File: {submission_filename}\")\nprint(f\"  - Format: ✅ Correct\")\nprint(f\"  - Ready for upload: ✅ Yes\")\n\nprint(\"\\nKey Features:\")\nprint(\"  ✅ Multi-target prediction (5 properties simultaneously)\")\nprint(\"  ✅ Missing value handling with binary masks\")\nprint(\"  ✅ Competition metric implementation (weighted MAE)\")\nprint(\"  ✅ Advanced molecular featurization (26 atom features)\")\nprint(\"  ✅ Early stopping and learning rate scheduling\")\nprint(\"  ✅ Comprehensive evaluation and visualization\")\n\nprint(\"=\"*80)\nprint(\"🎉 SOLUTION COMPLETE - READY FOR SUBMISSION! 🎉\")\nprint(\"=\"*80)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T14:13:32.784055Z","iopub.execute_input":"2025-08-03T14:13:32.784337Z","iopub.status.idle":"2025-08-03T14:13:32.796963Z","shell.execute_reply.started":"2025-08-03T14:13:32.784314Z","shell.execute_reply":"2025-08-03T14:13:32.795599Z"}},"outputs":[],"execution_count":null}]}